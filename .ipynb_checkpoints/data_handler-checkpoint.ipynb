{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple, defaultdict\n",
    "import codecs\n",
    "from config import config\n",
    "from bs4 import BeautifulSoup\n",
    "import pdb\n",
    "import torch\n",
    "from nltk.tokenize import word_tokenize\n",
    "#import tokenizer\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "##Added by Richard Sun\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "options_file = \"../data/Elmo/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
    "weight_file = \"../data/Elmo/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
    "elmo = Elmo(options_file, weight_file, 2, dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SentInst = namedtuple(\"SentenceInstance\", \"id text text_inds opinions\")\n",
    "OpinionInst = namedtuple(\"OpinionInstance\", \"target_text polarity class_ind target_mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataHelper():\n",
    "    def __init__(self, config):\n",
    "        '''\n",
    "        This class is able to:\n",
    "        1. Load datasets\n",
    "        2. Split sentences into words\n",
    "        3. Map words into Idx\n",
    "        '''\n",
    "        self.config = config\n",
    "\n",
    "        # id map to instance\n",
    "        self.id2label = [\"positive\", \"neutral\", \"negative\"]\n",
    "        self.label2id = {v:k for k,v in enumerate(self.id2label)}\n",
    "\n",
    "        self.UNK = \"UNK\"\n",
    "        self.EOS = \"EOS\"\n",
    "\n",
    "        # data\n",
    "        self.train_data = None\n",
    "        self.test_data = None\n",
    "    \n",
    "    def read_xml_data(self, file_name):\n",
    "        '''\n",
    "        Read XML data\n",
    "        '''\n",
    "        f = codecs.open(file_name, \"r\", encoding=\"utf-8\")\n",
    "        soup = BeautifulSoup(f.read(), \"lxml\")\n",
    "        sentence_tags = soup.find_all(\"sentence\")\n",
    "        sentence_list = []\n",
    "        for sent_tag in sentence_tags:\n",
    "            sent_id = sent_tag.attrs[\"id\"]\n",
    "            sent_text = sent_tag.find(\"text\").contents[0]\n",
    "            opinion_list = []\n",
    "            try:\n",
    "                asp_tag = sent_tag.find_all(\"aspectterms\")[0]\n",
    "            except:\n",
    "                # print \"{0} {1} has no opinions\".format(sent_id, sent_text)\n",
    "                #print(sent_tag)\n",
    "                continue\n",
    "            opinion_tags = asp_tag.find_all(\"aspectterm\")\n",
    "            for opinion_tag in opinion_tags:\n",
    "                term = opinion_tag.attrs[\"term\"]\n",
    "                if term not in sent_text: pdb.set_trace()\n",
    "                polarity = opinion_tag.attrs[\"polarity\"]\n",
    "                opinion_inst = OpinionInst(term, polarity, None, None)\n",
    "                opinion_list.append(opinion_inst)\n",
    "            sent_Inst = SentInst(sent_id, sent_text, None, opinion_list)\n",
    "            sentence_list.append(sent_Inst)\n",
    "\n",
    "        return sentence_list\n",
    "\n",
    "\n",
    "    def tokenize(self, sent_str):\n",
    "        '''\n",
    "        Split a sentence into tokens\n",
    "        '''\n",
    "        # return word_tokenize(sent_str)\n",
    "        sent_str = \" \".join(sent_str.split(\"-\"))\n",
    "        sent_str = \" \".join(sent_str.split(\"/\"))\n",
    "        sent_str = \" \".join(sent_str.split(\"!\"))\n",
    "        sent = nlp(sent_str)\n",
    "        return [item.text for item in sent]\n",
    "        \n",
    "    # namedtuple is protected!\n",
    "    def process_raw_data(self, data):\n",
    "        '''\n",
    "        Tokenize each sentence, compute aspect mask for each sentence\n",
    "        '''\n",
    "        sent_len = len(data)\n",
    "        print('Sentences Num:', sent_len)\n",
    "        for sent_i in np.arange(sent_len):\n",
    "            sent_inst = data[sent_i]\n",
    "            sent_tokens = self.tokenize(sent_inst.text)\n",
    "            sent_inst = sent_inst._replace(text_inds = sent_tokens)\n",
    "            opinion_list = []\n",
    "            opi_len = len(sent_inst.opinions)\n",
    "            for opi_i in np.arange(opi_len):\n",
    "                opi_inst = sent_inst.opinions[opi_i]\n",
    "\n",
    "                target = opi_inst.target_text\n",
    "                target_tokens = self.tokenize(target)\n",
    "                try:\n",
    "                    target_start = sent_tokens.index(target_tokens[0])\n",
    "                    target_end = sent_tokens[max(0, target_start - 1):].index(target_tokens[-1])  + max(0, target_start - 1)\n",
    "                except:\n",
    "                    #pdb.set_trace()\n",
    "                    print('Target error '+target_tokens[0])\n",
    "                    continue\n",
    "                    \n",
    "                if target_start < 0 or target_end < 0:\n",
    "                    #pdb.set_trace()\n",
    "                    print('Traget not in the vocabulary')\n",
    "                    continue\n",
    "                    \n",
    "                mask = [0] * len(sent_tokens)\n",
    "                for m_i in range(target_start, target_end + 1):\n",
    "                    mask[m_i] = 1\n",
    "\n",
    "                label = opi_inst.polarity\n",
    "                if label == \"conflict\":  continue  # ignore conflict ones\n",
    "                opi_inst = opi_inst._replace(class_ind = self.label2id[label])\n",
    "                opi_inst = opi_inst._replace(target_mask = mask)\n",
    "                opinion_list.append(opi_inst)\n",
    "            \n",
    "            sent_inst = sent_inst._replace(opinions = opinion_list)\n",
    "            \n",
    "            data[sent_i] = sent_inst\n",
    "        return data\n",
    "\n",
    "    \n",
    "    def read(self, train_data, test_data):\n",
    "        '''\n",
    "        Preprocess the data\n",
    "        '''\n",
    "        self.train_data = self.read_xml_data(train_data)\n",
    "        self.test_data = self.read_xml_data(test_data)\n",
    "        print('Training dataset number:', len(self.train_data))\n",
    "        print('Testing dataset number:', len(self.test_data))\n",
    "        train_data = self.process_raw_data(self.train_data)\n",
    "        test_data = self.process_raw_data(self.test_data)\n",
    "        return train_data, test_data\n",
    "    \n",
    "    def generate_sample(self, all_triples):\n",
    "        '''\n",
    "        Generate a batch of training dataset\n",
    "        '''\n",
    "        batch_size = self.config.batch_size\n",
    "        select_index = np.random.choice(len(all_triples), batch_size)\n",
    "        select_trip = [all_triples[i] for i in select_index]\n",
    "        return select_trip\n",
    "        \n",
    "\n",
    "    # shuffle and to batch size\n",
    "    def to_batches(self, data, if_batch = False):\n",
    "        all_triples = []\n",
    "        # list of list\n",
    "        pair_couter = defaultdict(int)\n",
    "        print(\"Sentence size \", len(data))\n",
    "        for sent_inst in data:\n",
    "            tokens = sent_inst.text_inds\n",
    "            #print(tokens)\n",
    "            for opi_inst in sent_inst.opinions:\n",
    "                if opi_inst.polarity is None:  continue # conflict one\n",
    "                mask = opi_inst.target_mask\n",
    "                polarity = opi_inst.class_ind\n",
    "                if tokens is None or mask is None or polarity is None: \n",
    "                    continue\n",
    "                all_triples.append([tokens, mask, polarity])\n",
    "                pair_couter[polarity] += 1\n",
    "                \n",
    "        print(pair_couter)\n",
    "\n",
    "        if if_batch:\n",
    "            print('Shuffle')\n",
    "            random.shuffle(all_triples)\n",
    "            batch_n = int(len(all_triples) / self.config.batch_size + 1)\n",
    "            print(\"{0} instances with {1} batches\".format(len(all_triples), batch_n))\n",
    "            ret_triples = []\n",
    "            \n",
    "            offset = 0\n",
    "            for i in range(batch_n):\n",
    "                start = self.config.batch_size * i\n",
    "                end = min(self.config.batch_size * (i+1), len(all_triples) )\n",
    "                ret_triples.append(all_triples[start : end])\n",
    "            return ret_triples\n",
    "        else:\n",
    "            return all_triples\n",
    "\n",
    "    \n",
    "    def debug_single_sample(self, batches, batch_n, sent_n):\n",
    "        sent_ind = batches[batch_n][sent_n][0]\n",
    "        print(\" \".join([self.id2word[x] for x in sent_ind]))\n",
    "        mask = batches[batch_n][sent_n][1]\n",
    "        print(mask)\n",
    "        label = batches[batch_n][sent_n][2]\n",
    "        print(self.id2label[label])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elmo_transform(triples):\n",
    "    '''\n",
    "    Split the triples into three lists\n",
    "    '''\n",
    "    token_list, mask_list, label_list = zip(*triples)\n",
    "    sent_lens = [len(tokens) for tokens in token_list]\n",
    "    max_len = max(sent_lens)\n",
    "    batch_size = len(sent_lens)\n",
    "    character_ids = batch_to_ids(token_list)\n",
    "    embeddings = elmo(character_ids)\n",
    "    #batch_size*word_num * 1024\n",
    "    sent_vecs = embeddings['elmo_representations'][0]\n",
    "    #Padding the mask to same lengths\n",
    "    mask_vecs = torch.zeros(batch_size, max_len)\n",
    "    for i, mask in enumerate(mask_list):\n",
    "        mask_vecs[i, :len(mask)] = torch.LongTensor(mask)\n",
    "    return sent_vecs, mask_vecs, label_list, sent_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = \"data/2014/Restaurants_Train_v2.xml\"\n",
    "TEST_DATA_PATH = \"data/2014/Restaurants_Test_Gold.xml\"\n",
    "\n",
    "# TRAIN_DATA_PATH = \"./data/2014/Laptop_Train_v2.xml\"\n",
    "# TEST_DATA_PATH = \"./data/2014/Laptops_Test_Gold.xml\"\n",
    "\n",
    "GLOVE_FILE = \"../data/word_embeddings/glove.6B.100d.txt\"\n",
    "OUT_FILE = config.embed_path\n",
    "DATA_FILE = config.data_path\n",
    "DIC_FILE = config.dic_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reader = dataHelper(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset number: 2021\n",
      "Testing dataset number: 606\n",
      "Sentences Num: 2021\n",
      "Sentences Num: 606\n",
      "Target error MEAL\n"
     ]
    }
   ],
   "source": [
    "train, test = reader.read(TRAIN_DATA_PATH, TEST_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence size  2021\n",
      "defaultdict(<class 'int'>, {2: 805, 0: 2164, 1: 633})\n",
      "Sentence size  606\n",
      "defaultdict(<class 'int'>, {0: 727, 1: 196, 2: 196})\n"
     ]
    }
   ],
   "source": [
    "#train_data = reader.read_data(TRAIN_DATA_PATH)\n",
    "#test_data = reader.read_data(TEST_DATA_PATH)\n",
    "train_batch = reader.to_batches(train)\n",
    "test_batch = reader.to_batches(test)\n",
    "# reader.debug_single_sample(train_batch, 0, 0)\n",
    "# pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = reader.generate_sample(train_batch)\n",
    "sent_vecs, mask_vecs, label_list, sent_lens = elmo_transform(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_vecs, mask_vecs, label_list, sent_lens = elmo_transform([test_batch[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write to pkl\n",
    "with open(DATA_FILE, \"wb\") as f:\n",
    "    pickle.dump([train_batch, test_batch],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with codecs.open(DIC_FILE, 'wb') as f:\n",
    "    pickle.dump(reader.id2word, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1411 unk out of 5138 vocab\n"
     ]
    }
   ],
   "source": [
    "reader.gen_vectors_glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Reader():\n",
    "    def __init__(self, config):\n",
    "        '''\n",
    "        This class is able to:\n",
    "        1. Load datasets\n",
    "        2. Split sentences into words\n",
    "        3. Map words into Idx\n",
    "        '''\n",
    "        self.config = config\n",
    "\n",
    "        # id map to instance\n",
    "        self.id2word = []\n",
    "        self.word2id = {}\n",
    "        self.id2label = [\"positive\", \"neutral\", \"negative\"]\n",
    "        self.label2id = {v:k for k,v in enumerate(self.id2label)}\n",
    "\n",
    "        self.UNK = \"UNK\"\n",
    "        self.EOS = \"EOS\"\n",
    "\n",
    "        # data\n",
    "        self.train_data = None\n",
    "        self.test_data = None\n",
    "    \n",
    "    def read_data(self, file_name):\n",
    "        f = codecs.open(file_name, \"r\", encoding=\"utf-8\")\n",
    "        soup = BeautifulSoup(f.read(), \"lxml\")\n",
    "        sentence_tags = soup.find_all(\"sentence\")\n",
    "\n",
    "        sentence_list = []\n",
    "        for sent_tag in sentence_tags:\n",
    "            sent_id = sent_tag.attrs[\"id\"]\n",
    "            sent_text = sent_tag.find(\"text\").contents[0]\n",
    "            opinion_list = []\n",
    "            try:\n",
    "                asp_tag = sent_tag.find_all(\"aspectterms\")[0]\n",
    "            except:\n",
    "                # print \"{0} {1} has no opinions\".format(sent_id, sent_text)\n",
    "                continue\n",
    "            opinion_tags = asp_tag.find_all(\"aspectterm\")\n",
    "            for opinion_tag in opinion_tags:\n",
    "                term = opinion_tag.attrs[\"term\"]\n",
    "                if term not in sent_text: pdb.set_trace()\n",
    "                polarity = opinion_tag.attrs[\"polarity\"]\n",
    "                opinion_inst = OpinionInst(term, polarity, None, None)\n",
    "                opinion_list.append(opinion_inst)\n",
    "            sent_Inst = SentInst(sent_id, sent_text, None, opinion_list)\n",
    "            sentence_list.append(sent_Inst)\n",
    "\n",
    "        return sentence_list\n",
    "\n",
    "    # generate vocabulary\n",
    "    def gen_dic(self):\n",
    "        words_set = set()\n",
    "        label_set = set()\n",
    "\n",
    "        # unknow\n",
    "        words_set.add(self.UNK)\n",
    "        #Is this data leakage\n",
    "        for data in [self.train_data, self.test_data]:\n",
    "            sent_counter = 0\n",
    "            for sent_inst in data:\n",
    "                sent_counter += 1\n",
    "                tokens = self.tokenize(sent_inst.text)\n",
    "                # pdb.set_trace()\n",
    "                for token in tokens:\n",
    "                    if token not in words_set:\n",
    "                        words_set.add(token)\n",
    "            print(\"{0} sentences\".format(sent_counter)) \n",
    "        self.id2word = list(words_set)\n",
    "        self.word2id = {v:k for k,v in enumerate(self.id2word)}\n",
    "\n",
    "        print(\"{0} tokens\".format(self.id2word.__len__()))\n",
    "\n",
    "    def tokenize(self, sent_str):\n",
    "        # return word_tokenize(sent_str)\n",
    "        sent_str = \" \".join(sent_str.split(\"-\"))\n",
    "        sent_str = \" \".join(sent_str.split(\"/\"))\n",
    "        sent_str = \" \".join(sent_str.split(\"!\"))\n",
    "        sent = nlp(sent_str)\n",
    "        return [item.text for item in sent]\n",
    "        \n",
    "    # namedtuple is protected!\n",
    "    def to_index(self, data):\n",
    "        sent_len = len(data)\n",
    "        for sent_i in range(sent_len):\n",
    "            sent_inst = data[sent_i]\n",
    "            sent_tokens = self.tokenize(sent_inst.text)\n",
    "#             If x in the vocabulary return its id, otherwise replace it with unknow\n",
    "#             sent_inds = [self.word2id[x] if x in self.word2id else self.word2id[self.UNK] \n",
    "#                 for x in sent_tokens]\n",
    "#             if sent_inds is None: \n",
    "#                 print('sentence is empty')\n",
    "#                 continue\n",
    "            #If we wanna use Elmo, comment the mapping    \n",
    "            #sent_inst = sent_inst._replace(text_inds = sent_inds)\n",
    "            sent_inst = sent_inst._replace(text_inds = sent_tokens)\n",
    "\n",
    "            opinion_list = []\n",
    "            opi_len = len(sent_inst.opinions)\n",
    "            for opi_i in range(opi_len):\n",
    "                opi_inst = sent_inst.opinions[opi_i]\n",
    "\n",
    "                target = opi_inst.target_text\n",
    "                target_tokens = self.tokenize(target)\n",
    "                try:\n",
    "                    target_start = sent_tokens.index(target_tokens[0])\n",
    "                    target_end = sent_tokens[max(0, target_start - 1):].index(target_tokens[-1])  + max(0, target_start - 1)\n",
    "                except:\n",
    "                    #pdb.set_trace()\n",
    "                    continue\n",
    "                    print('Target error'+target_tokens[0])\n",
    "                if target_start < 0 or target_end < 0:\n",
    "                    #pdb.set_trace()\n",
    "                    continue\n",
    "                    print('Traget not in the vocabulary')\n",
    "                mask = [0] * len(sent_tokens)\n",
    "                for m_i in range(target_start, target_end + 1):\n",
    "                    mask[m_i] = 1\n",
    "\n",
    "                label = opi_inst.polarity\n",
    "                if label == \"conflict\":  continue  # ignore conflict ones\n",
    "                opi_inst = opi_inst._replace(class_ind = self.label2id[label])\n",
    "                opi_inst = opi_inst._replace(target_mask = mask)\n",
    "                opinion_list.append(opi_inst)\n",
    "            \n",
    "            sent_inst = sent_inst._replace(opinions = opinion_list)\n",
    "            \n",
    "            data[sent_i] = sent_inst\n",
    "\n",
    "    \n",
    "    def read(self):\n",
    "        self.train_data = self.read_data(TRAIN_DATA_PATH)\n",
    "        self.test_data = self.read_data(TEST_DATA_PATH)\n",
    "        self.gen_dic()\n",
    "        self.to_index(self.train_data)\n",
    "        self.to_index(self.test_data)\n",
    "        return self.train_data, self.test_data\n",
    "\n",
    "    # shuffle and to batch size\n",
    "    def to_batches(self, data, if_batch = False):\n",
    "        all_triples = []\n",
    "        # list of list\n",
    "        pair_couter = defaultdict(int)\n",
    "        print(\"Sentence size \", len(data))\n",
    "        for sent_inst in data:\n",
    "            tokens = sent_inst.text_inds\n",
    "            \n",
    "            for opi_inst in sent_inst.opinions:\n",
    "                if opi_inst.polarity is None:  continue # conflict one\n",
    "                mask = opi_inst.target_mask\n",
    "                polarity = opi_inst.class_ind\n",
    "                if tokens is None or mask is None or polarity is None: pdb.set_trace()\n",
    "                all_triples.append([tokens, mask, polarity])\n",
    "                pair_couter[polarity] += 1\n",
    "        print(pair_couter)\n",
    "\n",
    "        if if_batch:\n",
    "            random.shuffle(all_triples)\n",
    "            batch_n = int(len(all_triples) / self.config.batch_size + 1)\n",
    "            print(\"{0} instances with {1} batches\".format(len(all_triples), batch_n))\n",
    "            ret_triples = []\n",
    "            \n",
    "            offset = 0\n",
    "            for i in range(batch_n):\n",
    "                start = self.config.batch_size * i\n",
    "                end = min(self.config.batch_size * (i+1), len(all_triples) )\n",
    "                ret_triples.append(all_triples[start : end])\n",
    "            return ret_triples\n",
    "        else:\n",
    "            return all_triples\n",
    "\n",
    "    def gen_vectors_glove(self):\n",
    "        vocab_dic = {}\n",
    "        with open(GLOVE_FILE) as f:\n",
    "            for line in f:\n",
    "                s_s = line.split()\n",
    "                if s_s[0] in self.word2id:\n",
    "                    vocab_dic[s_s[0]] = np.array([float(x) for x in s_s[1:]])\n",
    "\n",
    "        unknowns = np.random.uniform(-0.01, 0.01, config.embed_dim).astype(\"float32\")\n",
    "        ret_mat = []\n",
    "        unk_counter = 0\n",
    "        for token in self.id2word:\n",
    "            # token = token.lower()\n",
    "            if token in vocab_dic:\n",
    "                ret_mat.append(vocab_dic[token])\n",
    "            else:\n",
    "                ret_mat.append(unknowns)\n",
    "                # print token\n",
    "                unk_counter += 1\n",
    "        ret_mat = np.vstack(ret_mat)\n",
    "        with open(OUT_FILE, \"wb\") as f:\n",
    "            pickle.dump(ret_mat, f)\n",
    "        print(\"{0} unk out of {1} vocab\".format(unk_counter, len(self.id2word)))        \n",
    "    \n",
    "    def load_vectors(self):\n",
    "        with open(OUT_FILE, 'rb') as f:\n",
    "            self.id2vec = pickle.load(f)\n",
    "    \n",
    "    def debug_single_sample(self, batches, batch_n, sent_n):\n",
    "        sent_ind = batches[batch_n][sent_n][0]\n",
    "        print(\" \".join([self.id2word[x] for x in sent_ind]))\n",
    "        mask = batches[batch_n][sent_n][1]\n",
    "        print(mask)\n",
    "        label = batches[batch_n][sent_n][2]\n",
    "        print(self.id2label[label])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardsun/anaconda3/envs/allennlp/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "from model_att import *\n",
    "model = attTSA(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Argument 'other' has incorrect type (expected spacy.tokens.token.Token, got str)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-9c6a44ee2872>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;34m'food'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0;34m'food'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Argument 'other' has incorrect type (expected spacy.tokens.token.Token, got str)"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"To be completely fair, the only redeeming factor was the food, which was above average, but couldn't make up for all the other deficiencies of Teodora.\")\n",
    "target = 'food'\n",
    "for token in doc:\n",
    "    if 'food' in token.head.text:\n",
    "        print(token)\n",
    "    if 'food' in token.children:\n",
    "        print(token)\n",
    "#     print(token.text, token.dep_, token.head.text,\n",
    "#           [child for child in token.children])\n",
    "#     if token.text == 'food':\n",
    "#         a = token\n",
    "#     if token.text == 'deficiencies':\n",
    "#         b= token\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardsun/anaconda3/envs/allennlp/lib/python3.6/runpy.py:193: DeprecationWarning: [W003] Positional arguments to Doc.merge are deprecated. Instead, use the keyword arguments, for example tag=, lemma= or ent_type=.\n",
      "  \"__main__\", mod_spec)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'word': 'was',\n",
       "  'lemma': 'be',\n",
       "  'NE': '',\n",
       "  'POS_fine': 'VBD',\n",
       "  'POS_coarse': 'VERB',\n",
       "  'arc': 'ROOT',\n",
       "  'modifiers': [{'word': 'be',\n",
       "    'lemma': 'be',\n",
       "    'NE': '',\n",
       "    'POS_fine': 'VB',\n",
       "    'POS_coarse': 'VERB',\n",
       "    'arc': 'advcl',\n",
       "    'modifiers': [{'word': 'To',\n",
       "      'lemma': 'to',\n",
       "      'NE': '',\n",
       "      'POS_fine': 'TO',\n",
       "      'POS_coarse': 'PART',\n",
       "      'arc': 'aux',\n",
       "      'modifiers': []},\n",
       "     {'word': 'fair',\n",
       "      'lemma': 'fair',\n",
       "      'NE': '',\n",
       "      'POS_fine': 'JJ',\n",
       "      'POS_coarse': 'ADJ',\n",
       "      'arc': 'acomp',\n",
       "      'modifiers': [{'word': 'completely',\n",
       "        'lemma': 'completely',\n",
       "        'NE': '',\n",
       "        'POS_fine': 'RB',\n",
       "        'POS_coarse': 'ADV',\n",
       "        'arc': 'advmod',\n",
       "        'modifiers': []}]}]},\n",
       "   {'word': ',',\n",
       "    'lemma': ',',\n",
       "    'NE': '',\n",
       "    'POS_fine': ',',\n",
       "    'POS_coarse': 'PUNCT',\n",
       "    'arc': 'punct',\n",
       "    'modifiers': []},\n",
       "   {'word': 'factor',\n",
       "    'lemma': 'factor',\n",
       "    'NE': '',\n",
       "    'POS_fine': 'NN',\n",
       "    'POS_coarse': 'NOUN',\n",
       "    'arc': 'nsubj',\n",
       "    'modifiers': [{'word': 'the',\n",
       "      'lemma': 'the',\n",
       "      'NE': '',\n",
       "      'POS_fine': 'DT',\n",
       "      'POS_coarse': 'DET',\n",
       "      'arc': 'det',\n",
       "      'modifiers': []},\n",
       "     {'word': 'only',\n",
       "      'lemma': 'only',\n",
       "      'NE': '',\n",
       "      'POS_fine': 'JJ',\n",
       "      'POS_coarse': 'ADJ',\n",
       "      'arc': 'amod',\n",
       "      'modifiers': []},\n",
       "     {'word': 'redeeming',\n",
       "      'lemma': 'redeeming',\n",
       "      'NE': '',\n",
       "      'POS_fine': 'NN',\n",
       "      'POS_coarse': 'NOUN',\n",
       "      'arc': 'amod',\n",
       "      'modifiers': []}]},\n",
       "   {'word': 'food',\n",
       "    'lemma': 'food',\n",
       "    'NE': '',\n",
       "    'POS_fine': 'NN',\n",
       "    'POS_coarse': 'NOUN',\n",
       "    'arc': 'attr',\n",
       "    'modifiers': [{'word': 'the',\n",
       "      'lemma': 'the',\n",
       "      'NE': '',\n",
       "      'POS_fine': 'DT',\n",
       "      'POS_coarse': 'DET',\n",
       "      'arc': 'det',\n",
       "      'modifiers': []},\n",
       "     {'word': ',',\n",
       "      'lemma': ',',\n",
       "      'NE': '',\n",
       "      'POS_fine': ',',\n",
       "      'POS_coarse': 'PUNCT',\n",
       "      'arc': 'punct',\n",
       "      'modifiers': []},\n",
       "     {'word': 'was',\n",
       "      'lemma': 'be',\n",
       "      'NE': '',\n",
       "      'POS_fine': 'VBD',\n",
       "      'POS_coarse': 'VERB',\n",
       "      'arc': 'relcl',\n",
       "      'modifiers': [{'word': 'which',\n",
       "        'lemma': 'which',\n",
       "        'NE': '',\n",
       "        'POS_fine': 'WDT',\n",
       "        'POS_coarse': 'ADJ',\n",
       "        'arc': 'nsubj',\n",
       "        'modifiers': []},\n",
       "       {'word': 'above',\n",
       "        'lemma': 'above',\n",
       "        'NE': '',\n",
       "        'POS_fine': 'IN',\n",
       "        'POS_coarse': 'ADP',\n",
       "        'arc': 'prep',\n",
       "        'modifiers': [{'word': 'average',\n",
       "          'lemma': 'average',\n",
       "          'NE': '',\n",
       "          'POS_fine': 'JJ',\n",
       "          'POS_coarse': 'ADJ',\n",
       "          'arc': 'pobj',\n",
       "          'modifiers': []}]}]}]},\n",
       "   {'word': ',',\n",
       "    'lemma': ',',\n",
       "    'NE': '',\n",
       "    'POS_fine': ',',\n",
       "    'POS_coarse': 'PUNCT',\n",
       "    'arc': 'punct',\n",
       "    'modifiers': []},\n",
       "   {'word': 'but',\n",
       "    'lemma': 'but',\n",
       "    'NE': '',\n",
       "    'POS_fine': 'CC',\n",
       "    'POS_coarse': 'CCONJ',\n",
       "    'arc': 'cc',\n",
       "    'modifiers': []},\n",
       "   {'word': 'make',\n",
       "    'lemma': 'make',\n",
       "    'NE': '',\n",
       "    'POS_fine': 'VB',\n",
       "    'POS_coarse': 'VERB',\n",
       "    'arc': 'conj',\n",
       "    'modifiers': [{'word': 'could',\n",
       "      'lemma': 'could',\n",
       "      'NE': '',\n",
       "      'POS_fine': 'MD',\n",
       "      'POS_coarse': 'VERB',\n",
       "      'arc': 'aux',\n",
       "      'modifiers': []},\n",
       "     {'word': \"n't\",\n",
       "      'lemma': \"n't\",\n",
       "      'NE': '',\n",
       "      'POS_fine': 'RB',\n",
       "      'POS_coarse': 'ADV',\n",
       "      'arc': 'neg',\n",
       "      'modifiers': []},\n",
       "     {'word': 'up',\n",
       "      'lemma': 'up',\n",
       "      'NE': '',\n",
       "      'POS_fine': 'RP',\n",
       "      'POS_coarse': 'PART',\n",
       "      'arc': 'prt',\n",
       "      'modifiers': []},\n",
       "     {'word': 'for',\n",
       "      'lemma': 'for',\n",
       "      'NE': '',\n",
       "      'POS_fine': 'IN',\n",
       "      'POS_coarse': 'ADP',\n",
       "      'arc': 'prep',\n",
       "      'modifiers': [{'word': 'deficiencies',\n",
       "        'lemma': 'deficiency',\n",
       "        'NE': '',\n",
       "        'POS_fine': 'NNS',\n",
       "        'POS_coarse': 'NOUN',\n",
       "        'arc': 'pobj',\n",
       "        'modifiers': [{'word': 'all',\n",
       "          'lemma': 'all',\n",
       "          'NE': '',\n",
       "          'POS_fine': 'PDT',\n",
       "          'POS_coarse': 'ADJ',\n",
       "          'arc': 'predet',\n",
       "          'modifiers': []},\n",
       "         {'word': 'the',\n",
       "          'lemma': 'the',\n",
       "          'NE': '',\n",
       "          'POS_fine': 'DT',\n",
       "          'POS_coarse': 'DET',\n",
       "          'arc': 'det',\n",
       "          'modifiers': []},\n",
       "         {'word': 'other',\n",
       "          'lemma': 'other',\n",
       "          'NE': '',\n",
       "          'POS_fine': 'JJ',\n",
       "          'POS_coarse': 'ADJ',\n",
       "          'arc': 'amod',\n",
       "          'modifiers': []},\n",
       "         {'word': 'of',\n",
       "          'lemma': 'of',\n",
       "          'NE': '',\n",
       "          'POS_fine': 'IN',\n",
       "          'POS_coarse': 'ADP',\n",
       "          'arc': 'prep',\n",
       "          'modifiers': [{'word': 'Teodora',\n",
       "            'lemma': 'Teodora',\n",
       "            'NE': 'PERSON',\n",
       "            'POS_fine': 'NNP',\n",
       "            'POS_coarse': 'PROPN',\n",
       "            'arc': 'pobj',\n",
       "            'modifiers': []}]}]}]}]},\n",
       "   {'word': '.',\n",
       "    'lemma': '.',\n",
       "    'NE': '',\n",
       "    'POS_fine': '.',\n",
       "    'POS_coarse': 'PUNCT',\n",
       "    'arc': 'punct',\n",
       "    'modifiers': []}]}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['_',\n",
       " '__bytes__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__pyx_vtable__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__unicode__',\n",
       " 'ancestors',\n",
       " 'check_flag',\n",
       " 'children',\n",
       " 'cluster',\n",
       " 'conjuncts',\n",
       " 'dep',\n",
       " 'dep_',\n",
       " 'doc',\n",
       " 'ent_id',\n",
       " 'ent_id_',\n",
       " 'ent_iob',\n",
       " 'ent_iob_',\n",
       " 'ent_type',\n",
       " 'ent_type_',\n",
       " 'get_extension',\n",
       " 'has_extension',\n",
       " 'has_vector',\n",
       " 'head',\n",
       " 'i',\n",
       " 'idx',\n",
       " 'is_alpha',\n",
       " 'is_ancestor',\n",
       " 'is_ascii',\n",
       " 'is_bracket',\n",
       " 'is_currency',\n",
       " 'is_digit',\n",
       " 'is_left_punct',\n",
       " 'is_lower',\n",
       " 'is_oov',\n",
       " 'is_punct',\n",
       " 'is_quote',\n",
       " 'is_right_punct',\n",
       " 'is_sent_start',\n",
       " 'is_space',\n",
       " 'is_stop',\n",
       " 'is_title',\n",
       " 'is_upper',\n",
       " 'lang',\n",
       " 'lang_',\n",
       " 'left_edge',\n",
       " 'lefts',\n",
       " 'lemma',\n",
       " 'lemma_',\n",
       " 'lex_id',\n",
       " 'like_email',\n",
       " 'like_num',\n",
       " 'like_url',\n",
       " 'lower',\n",
       " 'lower_',\n",
       " 'n_lefts',\n",
       " 'n_rights',\n",
       " 'nbor',\n",
       " 'norm',\n",
       " 'norm_',\n",
       " 'orth',\n",
       " 'orth_',\n",
       " 'pos',\n",
       " 'pos_',\n",
       " 'prefix',\n",
       " 'prefix_',\n",
       " 'prob',\n",
       " 'rank',\n",
       " 'right_edge',\n",
       " 'rights',\n",
       " 'sent_start',\n",
       " 'sentiment',\n",
       " 'set_extension',\n",
       " 'shape',\n",
       " 'shape_',\n",
       " 'similarity',\n",
       " 'string',\n",
       " 'subtree',\n",
       " 'suffix',\n",
       " 'suffix_',\n",
       " 'tag',\n",
       " 'tag_',\n",
       " 'text',\n",
       " 'text_with_ws',\n",
       " 'vector',\n",
       " 'vector_norm',\n",
       " 'vocab',\n",
       " 'whitespace_']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allennlp",
   "language": "python",
   "name": "allennlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
