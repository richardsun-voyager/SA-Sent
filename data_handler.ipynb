{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple, defaultdict\n",
    "import codecs\n",
    "from config import config\n",
    "from bs4 import BeautifulSoup\n",
    "import pdb\n",
    "import torch\n",
    "from nltk.tokenize import word_tokenize\n",
    "#import tokenizer\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "##Added by Richard Sun\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "from data_reader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = \"data/2014/Restaurants_Train_v2.xml\"\n",
    "TEST_DATA_PATH = \"data/2014/Restaurants_Test_Gold.xml\"\n",
    "TRAIN_DATA_PATH = \"data/Indonesian/indo_tweets.csv\"\n",
    "\n",
    "# TRAIN_DATA_PATH = \"./data/2014/Laptop_Train_v2.xml\"\n",
    "# TEST_DATA_PATH = \"./data/2014/Laptops_Test_Gold.xml\"\n",
    "\n",
    "GLOVE_FILE = \"../data/word_embeddings/indo_vectors.txt\"\n",
    "OUT_FILE = config.embed_path\n",
    "DATA_FILE = config.data_path\n",
    "DIC_FILE = config.dic_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#!pip install xlrd\n",
    "import pandas as pd\n",
    "#help(pd.read_excel)\n",
    "data = pd.read_csv(TRAIN_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count        7132\n",
       "unique          3\n",
       "top       Neutral\n",
       "freq         3091\n",
       "Name: Sentiment, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Sentiment.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Username</th>\n",
       "      <th>Keyword</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Overal Sentiment</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6130</th>\n",
       "      <td>6130</td>\n",
       "      <td>ZUL_Hasan</td>\n",
       "      <td>ISIS</td>\n",
       "      <td>TIPS jadi saksi nikah : Jangan baper .. Sesama...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>remarks: the ISIS in this context does not ref...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6534</th>\n",
       "      <td>6534</td>\n",
       "      <td>tifsembiring</td>\n",
       "      <td>ISIS</td>\n",
       "      <td>ISIS ...? ada yg bilang \"Istri Sholihah Idaman...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>remarks: ISIS here does not refer to the Islam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6538</th>\n",
       "      <td>6538</td>\n",
       "      <td>tifsembiring</td>\n",
       "      <td>HTI</td>\n",
       "      <td>@rayestu kmbalikan pd hti nurani mas, kita kom...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>remarks: hti nurani = hati nurani, does not re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0      Username Keyword  \\\n",
       "6130        6130     ZUL_Hasan    ISIS   \n",
       "6534        6534  tifsembiring    ISIS   \n",
       "6538        6538  tifsembiring     HTI   \n",
       "\n",
       "                                                  Tweet Sentiment  \\\n",
       "6130  TIPS jadi saksi nikah : Jangan baper .. Sesama...       NaN   \n",
       "6534  ISIS ...? ada yg bilang \"Istri Sholihah Idaman...       NaN   \n",
       "6538  @rayestu kmbalikan pd hti nurani mas, kita kom...       NaN   \n",
       "\n",
       "     Overal Sentiment                                         Unnamed: 5  \n",
       "6130              NaN  remarks: the ISIS in this context does not ref...  \n",
       "6534              NaN  remarks: ISIS here does not refer to the Islam...  \n",
       "6538              NaN  remarks: hti nurani = hati nurani, does not re...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[pd.isnull(data[['Sentiment']]).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data.to_csv('data/Indonesian/indo_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dr = data_reader(config)\n",
    "dr.load_data(config.train_path)\n",
    "dr_valid = data_reader(config, False)\n",
    "dr_valid.load_data(config.valid_path)\n",
    "dr_test = data_reader(config, False)\n",
    "dr_test.load_data(config.test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples = dr.generate_sample(dr.data_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = ['ss', 'ff','df','gf']\n",
    "word_freq_pair = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {2: 805, 0: 2164, 1: 633})\n"
     ]
    }
   ],
   "source": [
    "data_batch = dh.to_batches(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset number: 606\n",
      "Target error meal\n",
      "Local Embeddings Saved!\n"
     ]
    }
   ],
   "source": [
    "data2 = dh.read(TEST_DATA_PATH, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = reader.generate_sample(train_batch)\n",
    "sent_vecs, mask_vecs, label_list, sent_lens = elmo_transform(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_vecs, mask_vecs, label_list, sent_lens = elmo_transform([test_batch[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write to pkl\n",
    "with open(DATA_FILE, \"wb\") as f:\n",
    "    pickle.dump([train_batch, test_batch],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with codecs.open(DIC_FILE, 'wb') as f:\n",
    "    pickle.dump(reader.id2word, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1411 unk out of 5138 vocab\n"
     ]
    }
   ],
   "source": [
    "reader.gen_vectors_glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardsun/anaconda3/envs/allennlp/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "from model_att import *\n",
    "model = attTSA(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:To Head:be Children: []\n",
      "Text:be Head:was Children: [To, fair]\n",
      "Text:completely Head:fair Children: []\n",
      "Text:fair Head:be Children: [completely]\n",
      "Text:, Head:was Children: []\n",
      "Text:the Head:factor Children: []\n",
      "Text:only Head:factor Children: []\n",
      "Text:redeeming Head:factor Children: []\n",
      "Text:factor Head:was Children: [the, only, redeeming]\n",
      "Text:was Head:was Children: [be, ,, factor, food, ,, but, make, .]\n",
      "Text:the Head:food Children: []\n",
      "Text:food Head:was Children: [the, ,, was]\n",
      "Text:, Head:food Children: []\n",
      "Text:which Head:was Children: []\n",
      "Text:was Head:food Children: [which, above]\n",
      "Text:above Head:was Children: [average]\n",
      "Text:average Head:above Children: []\n",
      "Text:, Head:was Children: []\n",
      "Text:but Head:was Children: []\n",
      "Text:could Head:make Children: []\n",
      "Text:n't Head:make Children: []\n",
      "Text:make Head:was Children: [could, n't, up, for]\n",
      "Text:up Head:make Children: []\n",
      "Text:for Head:make Children: [deficiencies]\n",
      "Text:all Head:deficiencies Children: []\n",
      "Text:the Head:deficiencies Children: []\n",
      "Text:other Head:deficiencies Children: []\n",
      "Text:deficiencies Head:for Children: [all, the, other, of]\n",
      "Text:of Head:deficiencies Children: [Teodora]\n",
      "Text:Teodora Head:of Children: []\n",
      "Text:. Head:was Children: []\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"To be completely fair, the only redeeming factor was the food, which was above average, but couldn't make up for all the other deficiencies of Teodora.\")\n",
    "target = 'food'\n",
    "for token in doc:\n",
    "#     if 'food' in token.head.text:\n",
    "#         print(token)\n",
    "#     children = [t.text for t in token.children]    \n",
    "#     if 'food' in children:\n",
    "#         print(token)\n",
    "    print('Text:'+ token.text, 'Head:'+token.head.text, 'Children:',\n",
    "           [child for child in token.children])\n",
    "#     if token.text == 'food':\n",
    "#         a = token\n",
    "#     if token.text == 'deficiencies':\n",
    "#         b= token\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be completely fair, the only redeeming factor was the food, which was above average, but couldn't make up for all the other deficiencies of Teodora.\n",
      "*******\n"
     ]
    }
   ],
   "source": [
    "for item in doc.sents:\n",
    "    print(item)\n",
    "    print('*******')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:the Head:screen Children: []\n",
      "Text:screen Head:is Children: [the]\n",
      "Text:is Head:is Children: [screen, good, but, sucks]\n",
      "Text:good Head:is Children: []\n",
      "Text:but Head:is Children: []\n",
      "Text:the Head:battery Children: []\n",
      "Text:battery Head:sucks Children: [the]\n",
      "Text:sucks Head:is Children: [battery]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'the screen is good but the battery sucks')\n",
    "for token in doc:\n",
    "#     if 'food' in token.head.text:\n",
    "#         print(token)\n",
    "#     children = [t.text for t in token.children]    \n",
    "#     if 'food' in children:\n",
    "#         print(token)\n",
    "    print('Text:'+ token.text, 'Head:'+token.head.text, 'Children:',\n",
    "           [child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"1450\" height=\"399.5\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">screen</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">good</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">but</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">battery</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">sucks</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M565.0,266.5 L573.0,254.5 557.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-3\" stroke-width=\"2px\" d=\"M420,264.5 C420,89.5 745.0,89.5 745.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,266.5 L753.0,254.5 737.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-4\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-5\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-5\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,266.5 L1112,254.5 1128,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-6\" stroke-width=\"2px\" d=\"M420,264.5 C420,2.0 1275.0,2.0 1275.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-6\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1275.0,266.5 L1283.0,254.5 1267.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style='dep', jupyter=True)\n",
    "# trees = doc.print_tree()\n",
    "# trees[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0x82 in position 6: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-35684d01c21d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/bailin_data/pre-trained-glove.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m             \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0x82 in position 6: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "with open('data/bailin_data/pre-trained-glove.pkl', 'rb') as f:\n",
    "            vectors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from model_glove import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from data/2014/pre-trained-glove.pkl with shape (5135, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardsun/anaconda3/envs/allennlp/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "model = attTSA(config)\n",
    "parameters = filter(lambda p: p.requires_grad, model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(data_path, if_utf=False):\n",
    "    f = open(data_path, 'rb')\n",
    "    obj = pickle.load(f)\n",
    "    f.close()\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_batch, test_batch = load_data('data/bailin_data/data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7678571428571429"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "860/len(test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for triple_list in test_batch:\n",
    "        model.zero_grad() \n",
    "        if len(triple_list) == 0: \n",
    "            continue\n",
    "        ##Modified by Richard Sun\n",
    "        sent, mask, label = triple_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 10, 64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "b = tf.Variable(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([1, 2])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {'a':1,'b':2}\n",
    "a.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/2014'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a='data/2014/a.txt'\n",
    "os.path.dirname(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_str = 'Klu si Tepen tak mau disebut teroris sebut Terochrist kek bro @realDonaldTrump pic.twitter.com/NXyERh0zuu'\n",
    "sent_str = \" \".join(sent_str.split(\"-\"))\n",
    "sent_str = \" \".join(sent_str.split(\"/\"))\n",
    "sent_str = \" \".join(sent_str.split(\"!\"))\n",
    "sent_str = \" \".join(sent_str.split(\"@\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Klu si Tepen tak mau disebut teroris sebut Terochrist kek bro  realDonaldTrump pic.twitter.com NXyERh0zuu'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = \"data/2014/Restaurants_Train_v2.xml\"\n",
    "TEST_DATA_PATH = \"data/2014/Restaurants_Test_Gold.xml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dh = dataHelper(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized Word Number: 4331\n",
      "Local Vocabulary Size: 4334\n"
     ]
    }
   ],
   "source": [
    "data1 = dh.read_xml_data(TRAIN_DATA_PATH)\n",
    "data_processed1, words1 = dh.process_raw_data(data1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error data: I HAVE NEVER HAD A BAD MEAL(OR BAD SERVICE )@ PIGALLE.\n",
      "Target error  ['MEAL']\n",
      "Tokenized Word Number: 2196\n",
      "Local Vocabulary Size: 2196\n"
     ]
    }
   ],
   "source": [
    "data2 = dh.read_xml_data(TEST_DATA_PATH)\n",
    "data_processed2, words2 = dh.process_raw_data(data2, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dr = data_reader(config)\n",
    "dr.load_data(config.train_path)\n",
    "dr_valid = data_reader(config, False)\n",
    "dr_valid.load_data(config.valid_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardsun/anaconda3/envs/allennlp/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "from Layer import GloveMaskCat\n",
    "from model_glove import *\n",
    "cat_layer = GloveMaskCat(config)\n",
    "model = attTSA(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from data/Indonesian/vocab/local_emb.pkl with shape (18117, 100)\n"
     ]
    }
   ],
   "source": [
    "cat_layer.load_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.1098)\n",
      "tensor(1.1083)\n",
      "tensor(1.1019)\n",
      "tensor(1.1059)\n",
      "tensor(1.1171)\n",
      "tensor(1.1137)\n",
      "tensor(1.1133)\n",
      "tensor(nan.)\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.0001)\n",
    "for _ in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    sent_vecs, mask_vecs, label_list, sent_lens = next(dr.get_ids_samples())\n",
    "    sent_vecs, target_avg = cat_layer(sent_vecs, mask_vecs)\n",
    "    cls_loss = model(sent_vecs, target_avg, label_list, sent_lens)\n",
    "    print(cls_loss)\n",
    "    if 'nan' in str(cls_loss) :\n",
    "        break\n",
    "    cls_loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.txt'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.path.basename(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__all__',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " '_get_sep',\n",
       " '_joinrealpath',\n",
       " '_varprog',\n",
       " '_varprogb',\n",
       " 'abspath',\n",
       " 'altsep',\n",
       " 'basename',\n",
       " 'commonpath',\n",
       " 'commonprefix',\n",
       " 'curdir',\n",
       " 'defpath',\n",
       " 'devnull',\n",
       " 'dirname',\n",
       " 'exists',\n",
       " 'expanduser',\n",
       " 'expandvars',\n",
       " 'extsep',\n",
       " 'genericpath',\n",
       " 'getatime',\n",
       " 'getctime',\n",
       " 'getmtime',\n",
       " 'getsize',\n",
       " 'isabs',\n",
       " 'isdir',\n",
       " 'isfile',\n",
       " 'islink',\n",
       " 'ismount',\n",
       " 'join',\n",
       " 'lexists',\n",
       " 'normcase',\n",
       " 'normpath',\n",
       " 'os',\n",
       " 'pardir',\n",
       " 'pathsep',\n",
       " 'realpath',\n",
       " 'relpath',\n",
       " 'samefile',\n",
       " 'sameopenfile',\n",
       " 'samestat',\n",
       " 'sep',\n",
       " 'split',\n",
       " 'splitdrive',\n",
       " 'splitext',\n",
       " 'stat',\n",
       " 'supports_unicode_filenames',\n",
       " 'sys']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "a = 'data/1.txt'\n",
    "dir(os.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-2.9648e-02, -4.4701e-01,  6.2723e-01,  ..., -1.3544e-01,\n",
       "           3.4295e-01, -8.3636e-02],\n",
       "         [-6.4288e-01, -1.6441e-01, -7.2294e-01,  ..., -4.4734e-01,\n",
       "           5.4922e-01,  9.7924e-01],\n",
       "         [ 4.2073e-01, -9.6298e-01, -2.7559e-02,  ...,  2.7644e-01,\n",
       "          -2.2384e-01, -1.3367e-01],\n",
       "         ...,\n",
       "         [ 4.0352e-01,  1.0910e-03, -3.6173e-01,  ..., -3.6537e-01,\n",
       "          -4.7223e-01, -4.7434e-01],\n",
       "         [-1.8483e-01,  6.8170e-03,  2.4457e-02,  ...,  1.6971e-01,\n",
       "          -7.7354e-01, -7.6071e-01],\n",
       "         [ 1.7269e-01,  1.0279e-01,  4.5758e-01,  ...,  2.5740e-01,\n",
       "          -9.9045e-01, -4.8982e-01]],\n",
       "\n",
       "        [[ 2.9311e-01, -1.1514e+00,  1.4071e+00,  ..., -1.0412e-02,\n",
       "          -3.0067e-02, -1.0123e+00],\n",
       "         [ 2.2465e-01, -7.7045e-01,  2.3710e-02,  ...,  1.1078e-01,\n",
       "           1.5709e-01, -6.4280e-01],\n",
       "         [ 3.6767e-01, -9.1452e-02,  8.8270e-02,  ...,  6.2300e-02,\n",
       "          -6.1650e-01,  1.0895e-02],\n",
       "         ...,\n",
       "         [-1.6259e-01,  1.1256e+00,  4.4490e-01,  ..., -4.2928e-01,\n",
       "           4.1126e-01, -5.6163e-01],\n",
       "         [-4.0210e-01, -5.5065e-01, -4.3386e-01,  ..., -6.6454e-02,\n",
       "           4.0114e-01, -2.6617e-01],\n",
       "         [-7.7062e-01,  1.2828e+00, -5.9122e-01,  ...,  1.3791e-01,\n",
       "           2.3606e-01, -3.4286e-01]],\n",
       "\n",
       "        [[-2.5038e-01, -5.3072e-01, -3.4788e-01,  ...,  2.8621e-02,\n",
       "           3.2361e-01,  2.0527e-01],\n",
       "         [-4.6766e-01, -1.4611e-02,  3.0691e-01,  ..., -1.4651e-01,\n",
       "          -4.5917e-01, -3.3930e-01],\n",
       "         [-2.3375e-01,  7.8533e-01,  3.0966e-01,  ...,  2.8365e-01,\n",
       "           1.4539e-01, -3.1865e-01],\n",
       "         ...,\n",
       "         [-8.2826e-01,  4.3010e-01,  4.9126e-01,  ...,  2.0627e-01,\n",
       "           5.2297e-01, -5.6761e-01],\n",
       "         [-7.7062e-01,  1.2828e+00, -5.9122e-01,  ...,  1.3791e-01,\n",
       "           2.3606e-01, -3.4286e-01],\n",
       "         [-7.7062e-01,  1.2828e+00, -5.9122e-01,  ...,  1.3791e-01,\n",
       "           2.3606e-01, -3.4286e-01]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[ 2.8315e-01, -1.3363e-01,  3.9460e-01,  ...,  6.7109e-01,\n",
       "           8.1850e-01,  5.6841e-01],\n",
       "         [ 2.2790e-01, -4.3015e-01,  2.8321e-01,  ...,  2.5255e-01,\n",
       "           2.4270e-01, -3.9811e-02],\n",
       "         [-1.9154e-01, -1.7787e-01, -3.9360e-03,  ...,  3.8730e-01,\n",
       "           1.2716e-01, -1.8357e-01],\n",
       "         ...,\n",
       "         [-7.7062e-01,  1.2828e+00, -5.9122e-01,  ...,  1.3791e-01,\n",
       "           2.3606e-01, -3.4286e-01],\n",
       "         [-7.7062e-01,  1.2828e+00, -5.9122e-01,  ...,  1.3791e-01,\n",
       "           2.3606e-01, -3.4286e-01],\n",
       "         [-7.7062e-01,  1.2828e+00, -5.9122e-01,  ...,  1.3791e-01,\n",
       "           2.3606e-01, -3.4286e-01]],\n",
       "\n",
       "        [[-7.7062e-01,  1.2828e+00, -5.9122e-01,  ...,  1.3791e-01,\n",
       "           2.3606e-01, -3.4286e-01],\n",
       "         [-7.7062e-01,  1.2828e+00, -5.9122e-01,  ...,  1.3791e-01,\n",
       "           2.3606e-01, -3.4286e-01],\n",
       "         [-2.2330e-01,  2.4440e-01,  7.4879e-01,  ...,  5.0597e-02,\n",
       "          -6.8113e-02, -5.7362e-02],\n",
       "         ...,\n",
       "         [-7.7062e-01,  1.2828e+00, -5.9122e-01,  ...,  1.3791e-01,\n",
       "           2.3606e-01, -3.4286e-01],\n",
       "         [-7.7062e-01,  1.2828e+00, -5.9122e-01,  ...,  1.3791e-01,\n",
       "           2.3606e-01, -3.4286e-01],\n",
       "         [-7.7062e-01,  1.2828e+00, -5.9122e-01,  ...,  1.3791e-01,\n",
       "           2.3606e-01, -3.4286e-01]],\n",
       "\n",
       "        [[-3.8097e-01,  9.3271e-01,  4.0253e-01,  ...,  3.2438e-02,\n",
       "           3.0544e-02,  1.1655e-01],\n",
       "         [ 4.3189e-01,  5.5477e-01,  5.6944e-01,  ...,  2.7617e-01,\n",
       "          -1.4863e-02,  6.3550e-02],\n",
       "         [-7.2621e-01,  7.5492e-01, -9.8172e-02,  ...,  4.6893e-01,\n",
       "           1.0295e-01, -3.1704e-01],\n",
       "         ...,\n",
       "         [-7.7062e-01,  1.2828e+00, -5.9122e-01,  ...,  1.3791e-01,\n",
       "           2.3606e-01, -3.4286e-01],\n",
       "         [-7.7062e-01,  1.2828e+00, -5.9122e-01,  ...,  1.3791e-01,\n",
       "           2.3606e-01, -3.4286e-01],\n",
       "         [-7.7062e-01,  1.2828e+00, -5.9122e-01,  ...,  1.3791e-01,\n",
       "           2.3606e-01, -3.4286e-01]]])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_vecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.0853)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pretrained_embed_path = \"../data/word_embeddings/vectors_processed_indo.txt\"\n",
    "def load_pretrained_word_emb(file_path):\n",
    "    '''\n",
    "    Load a specified vocabulary\n",
    "    '''\n",
    "    word_emb = {}\n",
    "    vocab_words = set()\n",
    "    with open(file_path) as fi:\n",
    "        for line in fi:\n",
    "            items = line.split()\n",
    "            word_emb[items[0]] = np.array(items[1:], dtype=np.float32)\n",
    "            vocab_words.add(items[0])\n",
    "    return word_emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "emb = load_pretrained_word_emb(pretrained_embed_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'unk'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-755ad5a17bc6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0memb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'unk'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m: 'unk'"
     ]
    }
   ],
   "source": [
    "emb.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Reader():\n",
    "    def __init__(self, config):\n",
    "        '''\n",
    "        This class is able to:\n",
    "        1. Load datasets\n",
    "        2. Split sentences into words\n",
    "        3. Map words into Idx\n",
    "        '''\n",
    "        self.config = config\n",
    "\n",
    "        # id map to instance\n",
    "        self.id2word = []\n",
    "        self.word2id = {}\n",
    "        self.id2label = [\"positive\", \"neutral\", \"negative\"]\n",
    "        self.label2id = {v:k for k,v in enumerate(self.id2label)}\n",
    "\n",
    "        self.UNK = \"UNK\"\n",
    "        self.EOS = \"EOS\"\n",
    "\n",
    "        # data\n",
    "        self.train_data = None\n",
    "        self.test_data = None\n",
    "    \n",
    "    def read_data(self, file_name):\n",
    "        f = codecs.open(file_name, \"r\", encoding=\"utf-8\")\n",
    "        soup = BeautifulSoup(f.read(), \"lxml\")\n",
    "        sentence_tags = soup.find_all(\"sentence\")\n",
    "\n",
    "        sentence_list = []\n",
    "        for sent_tag in sentence_tags:\n",
    "            sent_id = sent_tag.attrs[\"id\"]\n",
    "            sent_text = sent_tag.find(\"text\").contents[0]\n",
    "            opinion_list = []\n",
    "            try:\n",
    "                asp_tag = sent_tag.find_all(\"aspectterms\")[0]\n",
    "            except:\n",
    "                # print \"{0} {1} has no opinions\".format(sent_id, sent_text)\n",
    "                continue\n",
    "            opinion_tags = asp_tag.find_all(\"aspectterm\")\n",
    "            for opinion_tag in opinion_tags:\n",
    "                term = opinion_tag.attrs[\"term\"]\n",
    "                if term not in sent_text: pdb.set_trace()\n",
    "                polarity = opinion_tag.attrs[\"polarity\"]\n",
    "                opinion_inst = OpinionInst(term, polarity, None, None)\n",
    "                opinion_list.append(opinion_inst)\n",
    "            sent_Inst = SentInst(sent_id, sent_text, None, opinion_list)\n",
    "            sentence_list.append(sent_Inst)\n",
    "\n",
    "        return sentence_list\n",
    "\n",
    "    # generate vocabulary\n",
    "    def gen_dic(self):\n",
    "        words_set = set()\n",
    "        label_set = set()\n",
    "\n",
    "        # unknow\n",
    "        words_set.add(self.UNK)\n",
    "        #Is this data leakage\n",
    "        for data in [self.train_data, self.test_data]:\n",
    "            sent_counter = 0\n",
    "            for sent_inst in data:\n",
    "                sent_counter += 1\n",
    "                tokens = self.tokenize(sent_inst.text)\n",
    "                # pdb.set_trace()\n",
    "                for token in tokens:\n",
    "                    if token not in words_set:\n",
    "                        words_set.add(token)\n",
    "            print(\"{0} sentences\".format(sent_counter)) \n",
    "        self.id2word = list(words_set)\n",
    "        self.word2id = {v:k for k,v in enumerate(self.id2word)}\n",
    "\n",
    "        print(\"{0} tokens\".format(self.id2word.__len__()))\n",
    "\n",
    "    def tokenize(self, sent_str):\n",
    "        # return word_tokenize(sent_str)\n",
    "        sent_str = \" \".join(sent_str.split(\"-\"))\n",
    "        sent_str = \" \".join(sent_str.split(\"/\"))\n",
    "        sent_str = \" \".join(sent_str.split(\"!\"))\n",
    "        sent = nlp(sent_str)\n",
    "        return [item.text for item in sent]\n",
    "        \n",
    "    # namedtuple is protected!\n",
    "    def to_index(self, data):\n",
    "        sent_len = len(data)\n",
    "        for sent_i in range(sent_len):\n",
    "            sent_inst = data[sent_i]\n",
    "            sent_tokens = self.tokenize(sent_inst.text)\n",
    "#             If x in the vocabulary return its id, otherwise replace it with unknow\n",
    "#             sent_inds = [self.word2id[x] if x in self.word2id else self.word2id[self.UNK] \n",
    "#                 for x in sent_tokens]\n",
    "#             if sent_inds is None: \n",
    "#                 print('sentence is empty')\n",
    "#                 continue\n",
    "            #If we wanna use Elmo, comment the mapping    \n",
    "            #sent_inst = sent_inst._replace(text_inds = sent_inds)\n",
    "            sent_inst = sent_inst._replace(text_inds = sent_tokens)\n",
    "\n",
    "            opinion_list = []\n",
    "            opi_len = len(sent_inst.opinions)\n",
    "            for opi_i in range(opi_len):\n",
    "                opi_inst = sent_inst.opinions[opi_i]\n",
    "\n",
    "                target = opi_inst.target_text\n",
    "                target_tokens = self.tokenize(target)\n",
    "                try:\n",
    "                    target_start = sent_tokens.index(target_tokens[0])\n",
    "                    target_end = sent_tokens[max(0, target_start - 1):].index(target_tokens[-1])  + max(0, target_start - 1)\n",
    "                except:\n",
    "                    #pdb.set_trace()\n",
    "                    continue\n",
    "                    print('Target error'+target_tokens[0])\n",
    "                if target_start < 0 or target_end < 0:\n",
    "                    #pdb.set_trace()\n",
    "                    continue\n",
    "                    print('Traget not in the vocabulary')\n",
    "                mask = [0] * len(sent_tokens)\n",
    "                for m_i in range(target_start, target_end + 1):\n",
    "                    mask[m_i] = 1\n",
    "\n",
    "                label = opi_inst.polarity\n",
    "                if label == \"conflict\":  continue  # ignore conflict ones\n",
    "                opi_inst = opi_inst._replace(class_ind = self.label2id[label])\n",
    "                opi_inst = opi_inst._replace(target_mask = mask)\n",
    "                opinion_list.append(opi_inst)\n",
    "            \n",
    "            sent_inst = sent_inst._replace(opinions = opinion_list)\n",
    "            \n",
    "            data[sent_i] = sent_inst\n",
    "\n",
    "    \n",
    "    def read(self):\n",
    "        self.train_data = self.read_data(TRAIN_DATA_PATH)\n",
    "        self.test_data = self.read_data(TEST_DATA_PATH)\n",
    "        self.gen_dic()\n",
    "        self.to_index(self.train_data)\n",
    "        self.to_index(self.test_data)\n",
    "        return self.train_data, self.test_data\n",
    "\n",
    "    # shuffle and to batch size\n",
    "    def to_batches(self, data, if_batch = False):\n",
    "        all_triples = []\n",
    "        # list of list\n",
    "        pair_couter = defaultdict(int)\n",
    "        print(\"Sentence size \", len(data))\n",
    "        for sent_inst in data:\n",
    "            tokens = sent_inst.text_inds\n",
    "            \n",
    "            for opi_inst in sent_inst.opinions:\n",
    "                if opi_inst.polarity is None:  continue # conflict one\n",
    "                mask = opi_inst.target_mask\n",
    "                polarity = opi_inst.class_ind\n",
    "                if tokens is None or mask is None or polarity is None: pdb.set_trace()\n",
    "                all_triples.append([tokens, mask, polarity])\n",
    "                pair_couter[polarity] += 1\n",
    "        print(pair_couter)\n",
    "\n",
    "        if if_batch:\n",
    "            random.shuffle(all_triples)\n",
    "            batch_n = int(len(all_triples) / self.config.batch_size + 1)\n",
    "            print(\"{0} instances with {1} batches\".format(len(all_triples), batch_n))\n",
    "            ret_triples = []\n",
    "            \n",
    "            offset = 0\n",
    "            for i in range(batch_n):\n",
    "                start = self.config.batch_size * i\n",
    "                end = min(self.config.batch_size * (i+1), len(all_triples) )\n",
    "                ret_triples.append(all_triples[start : end])\n",
    "            return ret_triples\n",
    "        else:\n",
    "            return all_triples\n",
    "\n",
    "    def gen_vectors_glove(self):\n",
    "        vocab_dic = {}\n",
    "        with open(GLOVE_FILE) as f:\n",
    "            for line in f:\n",
    "                s_s = line.split()\n",
    "                if s_s[0] in self.word2id:\n",
    "                    vocab_dic[s_s[0]] = np.array([float(x) for x in s_s[1:]])\n",
    "\n",
    "        unknowns = np.random.uniform(-0.01, 0.01, config.embed_dim).astype(\"float32\")\n",
    "        ret_mat = []\n",
    "        unk_counter = 0\n",
    "        for token in self.id2word:\n",
    "            # token = token.lower()\n",
    "            if token in vocab_dic:\n",
    "                ret_mat.append(vocab_dic[token])\n",
    "            else:\n",
    "                ret_mat.append(unknowns)\n",
    "                # print token\n",
    "                unk_counter += 1\n",
    "        ret_mat = np.vstack(ret_mat)\n",
    "        with open(OUT_FILE, \"wb\") as f:\n",
    "            pickle.dump(ret_mat, f)\n",
    "        print(\"{0} unk out of {1} vocab\".format(unk_counter, len(self.id2word)))        \n",
    "    \n",
    "    def load_vectors(self):\n",
    "        with open(OUT_FILE, 'rb') as f:\n",
    "            self.id2vec = pickle.load(f)\n",
    "    \n",
    "    def debug_single_sample(self, batches, batch_n, sent_n):\n",
    "        sent_ind = batches[batch_n][sent_n][0]\n",
    "        print(\" \".join([self.id2word[x] for x in sent_ind]))\n",
    "        mask = batches[batch_n][sent_n][1]\n",
    "        print(mask)\n",
    "        label = batches[batch_n][sent_n][2]\n",
    "        print(self.id2label[label])\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "allennlp",
   "language": "python",
   "name": "allennlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
