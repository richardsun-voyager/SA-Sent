{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import namedtuple, defaultdict\n",
    "import codecs\n",
    "from config import config\n",
    "from bs4 import BeautifulSoup\n",
    "import pdb\n",
    "import torch\n",
    "from nltk.tokenize import word_tokenize\n",
    "#import tokenizer\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "##Added by Richard Sun\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "options_file = \"../data/Elmo/elmo_2x4096_512_2048cnn_2xhighway_options.json\"\n",
    "weight_file = \"../data/Elmo/elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5\"\n",
    "elmo = Elmo(options_file, weight_file, 2, dropout=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SentInst = namedtuple(\"SentenceInstance\", \"id text text_inds opinions\")\n",
    "OpinionInst = namedtuple(\"OpinionInstance\", \"target_text polarity class_ind target_mask\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class dataHelper():\n",
    "    def __init__(self, config):\n",
    "        '''\n",
    "        This class is able to:\n",
    "        1. Load datasets\n",
    "        2. Split sentences into words\n",
    "        3. Map words into Idx\n",
    "        '''\n",
    "        self.config = config\n",
    "\n",
    "        # id map to instance\n",
    "        self.id2label = [\"positive\", \"neutral\", \"negative\"]\n",
    "        self.label2id = {v:k for k,v in enumerate(self.id2label)}\n",
    "\n",
    "        self.UNK = \"UNK\"\n",
    "        self.EOS = \"EOS\"\n",
    "\n",
    "        # data\n",
    "        self.train_data = None\n",
    "        self.test_data = None\n",
    "    \n",
    "    def read_xml_data(self, file_name):\n",
    "        '''\n",
    "        Read XML data\n",
    "        '''\n",
    "        f = codecs.open(file_name, \"r\", encoding=\"utf-8\")\n",
    "        soup = BeautifulSoup(f.read(), \"lxml\")\n",
    "        sentence_tags = soup.find_all(\"sentence\")\n",
    "        sentence_list = []\n",
    "        for sent_tag in sentence_tags:\n",
    "            sent_id = sent_tag.attrs[\"id\"]\n",
    "            sent_text = sent_tag.find(\"text\").contents[0]\n",
    "            opinion_list = []\n",
    "            try:\n",
    "                asp_tag = sent_tag.find_all(\"aspectterms\")[0]\n",
    "            except:\n",
    "                # print \"{0} {1} has no opinions\".format(sent_id, sent_text)\n",
    "                #print(sent_tag)\n",
    "                continue\n",
    "            opinion_tags = asp_tag.find_all(\"aspectterm\")\n",
    "            for opinion_tag in opinion_tags:\n",
    "                term = opinion_tag.attrs[\"term\"]\n",
    "                if term not in sent_text: pdb.set_trace()\n",
    "                polarity = opinion_tag.attrs[\"polarity\"]\n",
    "                opinion_inst = OpinionInst(term, polarity, None, None)\n",
    "                opinion_list.append(opinion_inst)\n",
    "            sent_Inst = SentInst(sent_id, sent_text, None, opinion_list)\n",
    "            sentence_list.append(sent_Inst)\n",
    "\n",
    "        return sentence_list\n",
    "\n",
    "\n",
    "    def tokenize(self, sent_str):\n",
    "        '''\n",
    "        Split a sentence into tokens\n",
    "        '''\n",
    "        # return word_tokenize(sent_str)\n",
    "        sent_str = \" \".join(sent_str.split(\"-\"))\n",
    "        sent_str = \" \".join(sent_str.split(\"/\"))\n",
    "        sent_str = \" \".join(sent_str.split(\"!\"))\n",
    "        sent = nlp(sent_str)\n",
    "        return [item.text for item in sent]\n",
    "        \n",
    "    # namedtuple is protected!\n",
    "    def process_raw_data(self, data):\n",
    "        '''\n",
    "        Tokenize each sentence, compute aspect mask for each sentence\n",
    "        '''\n",
    "        sent_len = len(data)\n",
    "        print('Sentences Num:', sent_len)\n",
    "        for sent_i in np.arange(sent_len):\n",
    "            sent_inst = data[sent_i]\n",
    "            sent_tokens = self.tokenize(sent_inst.text)\n",
    "            sent_inst = sent_inst._replace(text_inds = sent_tokens)\n",
    "            opinion_list = []\n",
    "            opi_len = len(sent_inst.opinions)\n",
    "            for opi_i in np.arange(opi_len):\n",
    "                opi_inst = sent_inst.opinions[opi_i]\n",
    "\n",
    "                target = opi_inst.target_text\n",
    "                target_tokens = self.tokenize(target)\n",
    "                try:\n",
    "                    target_start = sent_tokens.index(target_tokens[0])\n",
    "                    target_end = sent_tokens[max(0, target_start - 1):].index(target_tokens[-1])  + max(0, target_start - 1)\n",
    "                except:\n",
    "                    #pdb.set_trace()\n",
    "                    print('Target error '+target_tokens[0])\n",
    "                    continue\n",
    "                    \n",
    "                if target_start < 0 or target_end < 0:\n",
    "                    #pdb.set_trace()\n",
    "                    print('Traget not in the vocabulary')\n",
    "                    continue\n",
    "                    \n",
    "                mask = [0] * len(sent_tokens)\n",
    "                for m_i in range(target_start, target_end + 1):\n",
    "                    mask[m_i] = 1\n",
    "\n",
    "                label = opi_inst.polarity\n",
    "                if label == \"conflict\":  continue  # ignore conflict ones\n",
    "                opi_inst = opi_inst._replace(class_ind = self.label2id[label])\n",
    "                opi_inst = opi_inst._replace(target_mask = mask)\n",
    "                opinion_list.append(opi_inst)\n",
    "            \n",
    "            sent_inst = sent_inst._replace(opinions = opinion_list)\n",
    "            \n",
    "            data[sent_i] = sent_inst\n",
    "        return data\n",
    "\n",
    "    \n",
    "    def read(self, train_data, test_data):\n",
    "        '''\n",
    "        Preprocess the data\n",
    "        '''\n",
    "        self.train_data = self.read_xml_data(train_data)\n",
    "        self.test_data = self.read_xml_data(test_data)\n",
    "        print('Training dataset number:', len(self.train_data))\n",
    "        print('Testing dataset number:', len(self.test_data))\n",
    "        train_data = self.process_raw_data(self.train_data)\n",
    "        test_data = self.process_raw_data(self.test_data)\n",
    "        return train_data, test_data\n",
    "    \n",
    "    def generate_sample(self, all_triples):\n",
    "        '''\n",
    "        Generate a batch of training dataset\n",
    "        '''\n",
    "        batch_size = self.config.batch_size\n",
    "        select_index = np.random.choice(len(all_triples), batch_size)\n",
    "        select_trip = [all_triples[i] for i in select_index]\n",
    "        return select_trip\n",
    "        \n",
    "\n",
    "    # shuffle and to batch size\n",
    "    def to_batches(self, data, if_batch = False):\n",
    "        all_triples = []\n",
    "        # list of list\n",
    "        pair_couter = defaultdict(int)\n",
    "        print(\"Sentence size \", len(data))\n",
    "        for sent_inst in data:\n",
    "            tokens = sent_inst.text_inds\n",
    "            #print(tokens)\n",
    "            for opi_inst in sent_inst.opinions:\n",
    "                if opi_inst.polarity is None:  continue # conflict one\n",
    "                mask = opi_inst.target_mask\n",
    "                polarity = opi_inst.class_ind\n",
    "                if tokens is None or mask is None or polarity is None: \n",
    "                    continue\n",
    "                all_triples.append([tokens, mask, polarity])\n",
    "                pair_couter[polarity] += 1\n",
    "                \n",
    "        print(pair_couter)\n",
    "\n",
    "        if if_batch:\n",
    "            print('Shuffle')\n",
    "            random.shuffle(all_triples)\n",
    "            batch_n = int(len(all_triples) / self.config.batch_size + 1)\n",
    "            print(\"{0} instances with {1} batches\".format(len(all_triples), batch_n))\n",
    "            ret_triples = []\n",
    "            \n",
    "            offset = 0\n",
    "            for i in range(batch_n):\n",
    "                start = self.config.batch_size * i\n",
    "                end = min(self.config.batch_size * (i+1), len(all_triples) )\n",
    "                ret_triples.append(all_triples[start : end])\n",
    "            return ret_triples\n",
    "        else:\n",
    "            return all_triples\n",
    "\n",
    "    \n",
    "    def debug_single_sample(self, batches, batch_n, sent_n):\n",
    "        sent_ind = batches[batch_n][sent_n][0]\n",
    "        print(\" \".join([self.id2word[x] for x in sent_ind]))\n",
    "        mask = batches[batch_n][sent_n][1]\n",
    "        print(mask)\n",
    "        label = batches[batch_n][sent_n][2]\n",
    "        print(self.id2label[label])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def elmo_transform(triples):\n",
    "    '''\n",
    "    Split the triples into three lists\n",
    "    '''\n",
    "    token_list, mask_list, label_list = zip(*triples)\n",
    "    sent_lens = [len(tokens) for tokens in token_list]\n",
    "    max_len = max(sent_lens)\n",
    "    batch_size = len(sent_lens)\n",
    "    character_ids = batch_to_ids(token_list)\n",
    "    embeddings = elmo(character_ids)\n",
    "    #batch_size*word_num * 1024\n",
    "    sent_vecs = embeddings['elmo_representations'][0]\n",
    "    #Padding the mask to same lengths\n",
    "    mask_vecs = torch.zeros(batch_size, max_len)\n",
    "    for i, mask in enumerate(mask_list):\n",
    "        mask_vecs[i, :len(mask)] = torch.LongTensor(mask)\n",
    "    return sent_vecs, mask_vecs, label_list, sent_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = \"data/2014/Restaurants_Train_v2.xml\"\n",
    "TEST_DATA_PATH = \"data/2014/Restaurants_Test_Gold.xml\"\n",
    "\n",
    "# TRAIN_DATA_PATH = \"./data/2014/Laptop_Train_v2.xml\"\n",
    "# TEST_DATA_PATH = \"./data/2014/Laptops_Test_Gold.xml\"\n",
    "\n",
    "GLOVE_FILE = \"../data/word_embeddings/glove.6B.100d.txt\"\n",
    "OUT_FILE = config.embed_path\n",
    "DATA_FILE = config.data_path\n",
    "DIC_FILE = config.dic_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reader = dataHelper(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset number: 2021\n",
      "Testing dataset number: 606\n",
      "Sentences Num: 2021\n",
      "Sentences Num: 606\n",
      "Target error MEAL\n"
     ]
    }
   ],
   "source": [
    "train, test = reader.read(TRAIN_DATA_PATH, TEST_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence size  2021\n",
      "defaultdict(<class 'int'>, {2: 805, 0: 2164, 1: 633})\n",
      "Sentence size  606\n",
      "defaultdict(<class 'int'>, {0: 727, 1: 196, 2: 196})\n"
     ]
    }
   ],
   "source": [
    "#train_data = reader.read_data(TRAIN_DATA_PATH)\n",
    "#test_data = reader.read_data(TEST_DATA_PATH)\n",
    "train_batch = reader.to_batches(train)\n",
    "test_batch = reader.to_batches(test)\n",
    "# reader.debug_single_sample(train_batch, 0, 0)\n",
    "# pdb.set_trace()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sample = reader.generate_sample(train_batch)\n",
    "sent_vecs, mask_vecs, label_list, sent_lens = elmo_transform(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sent_vecs, mask_vecs, label_list, sent_lens = elmo_transform([test_batch[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# write to pkl\n",
    "with open(DATA_FILE, \"wb\") as f:\n",
    "    pickle.dump([train_batch, test_batch],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with codecs.open(DIC_FILE, 'wb') as f:\n",
    "    pickle.dump(reader.id2word, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1411 unk out of 5138 vocab\n"
     ]
    }
   ],
   "source": [
    "reader.gen_vectors_glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Reader():\n",
    "    def __init__(self, config):\n",
    "        '''\n",
    "        This class is able to:\n",
    "        1. Load datasets\n",
    "        2. Split sentences into words\n",
    "        3. Map words into Idx\n",
    "        '''\n",
    "        self.config = config\n",
    "\n",
    "        # id map to instance\n",
    "        self.id2word = []\n",
    "        self.word2id = {}\n",
    "        self.id2label = [\"positive\", \"neutral\", \"negative\"]\n",
    "        self.label2id = {v:k for k,v in enumerate(self.id2label)}\n",
    "\n",
    "        self.UNK = \"UNK\"\n",
    "        self.EOS = \"EOS\"\n",
    "\n",
    "        # data\n",
    "        self.train_data = None\n",
    "        self.test_data = None\n",
    "    \n",
    "    def read_data(self, file_name):\n",
    "        f = codecs.open(file_name, \"r\", encoding=\"utf-8\")\n",
    "        soup = BeautifulSoup(f.read(), \"lxml\")\n",
    "        sentence_tags = soup.find_all(\"sentence\")\n",
    "\n",
    "        sentence_list = []\n",
    "        for sent_tag in sentence_tags:\n",
    "            sent_id = sent_tag.attrs[\"id\"]\n",
    "            sent_text = sent_tag.find(\"text\").contents[0]\n",
    "            opinion_list = []\n",
    "            try:\n",
    "                asp_tag = sent_tag.find_all(\"aspectterms\")[0]\n",
    "            except:\n",
    "                # print \"{0} {1} has no opinions\".format(sent_id, sent_text)\n",
    "                continue\n",
    "            opinion_tags = asp_tag.find_all(\"aspectterm\")\n",
    "            for opinion_tag in opinion_tags:\n",
    "                term = opinion_tag.attrs[\"term\"]\n",
    "                if term not in sent_text: pdb.set_trace()\n",
    "                polarity = opinion_tag.attrs[\"polarity\"]\n",
    "                opinion_inst = OpinionInst(term, polarity, None, None)\n",
    "                opinion_list.append(opinion_inst)\n",
    "            sent_Inst = SentInst(sent_id, sent_text, None, opinion_list)\n",
    "            sentence_list.append(sent_Inst)\n",
    "\n",
    "        return sentence_list\n",
    "\n",
    "    # generate vocabulary\n",
    "    def gen_dic(self):\n",
    "        words_set = set()\n",
    "        label_set = set()\n",
    "\n",
    "        # unknow\n",
    "        words_set.add(self.UNK)\n",
    "        #Is this data leakage\n",
    "        for data in [self.train_data, self.test_data]:\n",
    "            sent_counter = 0\n",
    "            for sent_inst in data:\n",
    "                sent_counter += 1\n",
    "                tokens = self.tokenize(sent_inst.text)\n",
    "                # pdb.set_trace()\n",
    "                for token in tokens:\n",
    "                    if token not in words_set:\n",
    "                        words_set.add(token)\n",
    "            print(\"{0} sentences\".format(sent_counter)) \n",
    "        self.id2word = list(words_set)\n",
    "        self.word2id = {v:k for k,v in enumerate(self.id2word)}\n",
    "\n",
    "        print(\"{0} tokens\".format(self.id2word.__len__()))\n",
    "\n",
    "    def tokenize(self, sent_str):\n",
    "        # return word_tokenize(sent_str)\n",
    "        sent_str = \" \".join(sent_str.split(\"-\"))\n",
    "        sent_str = \" \".join(sent_str.split(\"/\"))\n",
    "        sent_str = \" \".join(sent_str.split(\"!\"))\n",
    "        sent = nlp(sent_str)\n",
    "        return [item.text for item in sent]\n",
    "        \n",
    "    # namedtuple is protected!\n",
    "    def to_index(self, data):\n",
    "        sent_len = len(data)\n",
    "        for sent_i in range(sent_len):\n",
    "            sent_inst = data[sent_i]\n",
    "            sent_tokens = self.tokenize(sent_inst.text)\n",
    "#             If x in the vocabulary return its id, otherwise replace it with unknow\n",
    "#             sent_inds = [self.word2id[x] if x in self.word2id else self.word2id[self.UNK] \n",
    "#                 for x in sent_tokens]\n",
    "#             if sent_inds is None: \n",
    "#                 print('sentence is empty')\n",
    "#                 continue\n",
    "            #If we wanna use Elmo, comment the mapping    \n",
    "            #sent_inst = sent_inst._replace(text_inds = sent_inds)\n",
    "            sent_inst = sent_inst._replace(text_inds = sent_tokens)\n",
    "\n",
    "            opinion_list = []\n",
    "            opi_len = len(sent_inst.opinions)\n",
    "            for opi_i in range(opi_len):\n",
    "                opi_inst = sent_inst.opinions[opi_i]\n",
    "\n",
    "                target = opi_inst.target_text\n",
    "                target_tokens = self.tokenize(target)\n",
    "                try:\n",
    "                    target_start = sent_tokens.index(target_tokens[0])\n",
    "                    target_end = sent_tokens[max(0, target_start - 1):].index(target_tokens[-1])  + max(0, target_start - 1)\n",
    "                except:\n",
    "                    #pdb.set_trace()\n",
    "                    continue\n",
    "                    print('Target error'+target_tokens[0])\n",
    "                if target_start < 0 or target_end < 0:\n",
    "                    #pdb.set_trace()\n",
    "                    continue\n",
    "                    print('Traget not in the vocabulary')\n",
    "                mask = [0] * len(sent_tokens)\n",
    "                for m_i in range(target_start, target_end + 1):\n",
    "                    mask[m_i] = 1\n",
    "\n",
    "                label = opi_inst.polarity\n",
    "                if label == \"conflict\":  continue  # ignore conflict ones\n",
    "                opi_inst = opi_inst._replace(class_ind = self.label2id[label])\n",
    "                opi_inst = opi_inst._replace(target_mask = mask)\n",
    "                opinion_list.append(opi_inst)\n",
    "            \n",
    "            sent_inst = sent_inst._replace(opinions = opinion_list)\n",
    "            \n",
    "            data[sent_i] = sent_inst\n",
    "\n",
    "    \n",
    "    def read(self):\n",
    "        self.train_data = self.read_data(TRAIN_DATA_PATH)\n",
    "        self.test_data = self.read_data(TEST_DATA_PATH)\n",
    "        self.gen_dic()\n",
    "        self.to_index(self.train_data)\n",
    "        self.to_index(self.test_data)\n",
    "        return self.train_data, self.test_data\n",
    "\n",
    "    # shuffle and to batch size\n",
    "    def to_batches(self, data, if_batch = False):\n",
    "        all_triples = []\n",
    "        # list of list\n",
    "        pair_couter = defaultdict(int)\n",
    "        print(\"Sentence size \", len(data))\n",
    "        for sent_inst in data:\n",
    "            tokens = sent_inst.text_inds\n",
    "            \n",
    "            for opi_inst in sent_inst.opinions:\n",
    "                if opi_inst.polarity is None:  continue # conflict one\n",
    "                mask = opi_inst.target_mask\n",
    "                polarity = opi_inst.class_ind\n",
    "                if tokens is None or mask is None or polarity is None: pdb.set_trace()\n",
    "                all_triples.append([tokens, mask, polarity])\n",
    "                pair_couter[polarity] += 1\n",
    "        print(pair_couter)\n",
    "\n",
    "        if if_batch:\n",
    "            random.shuffle(all_triples)\n",
    "            batch_n = int(len(all_triples) / self.config.batch_size + 1)\n",
    "            print(\"{0} instances with {1} batches\".format(len(all_triples), batch_n))\n",
    "            ret_triples = []\n",
    "            \n",
    "            offset = 0\n",
    "            for i in range(batch_n):\n",
    "                start = self.config.batch_size * i\n",
    "                end = min(self.config.batch_size * (i+1), len(all_triples) )\n",
    "                ret_triples.append(all_triples[start : end])\n",
    "            return ret_triples\n",
    "        else:\n",
    "            return all_triples\n",
    "\n",
    "    def gen_vectors_glove(self):\n",
    "        vocab_dic = {}\n",
    "        with open(GLOVE_FILE) as f:\n",
    "            for line in f:\n",
    "                s_s = line.split()\n",
    "                if s_s[0] in self.word2id:\n",
    "                    vocab_dic[s_s[0]] = np.array([float(x) for x in s_s[1:]])\n",
    "\n",
    "        unknowns = np.random.uniform(-0.01, 0.01, config.embed_dim).astype(\"float32\")\n",
    "        ret_mat = []\n",
    "        unk_counter = 0\n",
    "        for token in self.id2word:\n",
    "            # token = token.lower()\n",
    "            if token in vocab_dic:\n",
    "                ret_mat.append(vocab_dic[token])\n",
    "            else:\n",
    "                ret_mat.append(unknowns)\n",
    "                # print token\n",
    "                unk_counter += 1\n",
    "        ret_mat = np.vstack(ret_mat)\n",
    "        with open(OUT_FILE, \"wb\") as f:\n",
    "            pickle.dump(ret_mat, f)\n",
    "        print(\"{0} unk out of {1} vocab\".format(unk_counter, len(self.id2word)))        \n",
    "    \n",
    "    def load_vectors(self):\n",
    "        with open(OUT_FILE, 'rb') as f:\n",
    "            self.id2vec = pickle.load(f)\n",
    "    \n",
    "    def debug_single_sample(self, batches, batch_n, sent_n):\n",
    "        sent_ind = batches[batch_n][sent_n][0]\n",
    "        print(\" \".join([self.id2word[x] for x in sent_ind]))\n",
    "        mask = batches[batch_n][sent_n][1]\n",
    "        print(mask)\n",
    "        label = batches[batch_n][sent_n][2]\n",
    "        print(self.id2label[label])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardsun/anaconda3/envs/allennlp/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "from model_att import *\n",
    "model = attTSA(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be []\n",
      "be was [To, fair]\n",
      "completely fair []\n",
      "fair be [completely]\n",
      ", was []\n",
      "the factor []\n",
      "only factor []\n",
      "redeeming factor []\n",
      "factor was [the, only, redeeming]\n",
      "was was [be, ,, factor, food, ,, but, make, .]\n",
      "the food []\n",
      "food was [the, ,, was]\n",
      ", food []\n",
      "which was []\n",
      "was food [which, above]\n",
      "above was [average]\n",
      "average above []\n",
      ", was []\n",
      "but was []\n",
      "could make []\n",
      "n't make []\n",
      "make was [could, n't, up, for]\n",
      "up make []\n",
      "for make [deficiencies]\n",
      "all deficiencies []\n",
      "the deficiencies []\n",
      "other deficiencies []\n",
      "deficiencies for [all, the, other, of]\n",
      "of deficiencies [Teodora]\n",
      "Teodora of []\n",
      ". was []\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"To be completely fair, the only redeeming factor was the food, which was above average, but couldn't make up for all the other deficiencies of Teodora.\")\n",
    "target = 'food'\n",
    "for token in doc:\n",
    "#     if 'food' in token.head.text:\n",
    "#         print(token)\n",
    "#     children = [t.text for t in token.children]    \n",
    "#     if 'food' in children:\n",
    "#         print(token)\n",
    "    print(token.text,token.head.text,\n",
    "           [child for child in token.children])\n",
    "#     if token.text == 'food':\n",
    "#         a = token\n",
    "#     if token.text == 'deficiencies':\n",
    "#         b= token\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the only redeeming factor\n",
      "the food\n",
      "all the other deficiencies\n",
      "Teodora\n"
     ]
    }
   ],
   "source": [
    "for item in doc.noun_chunks:\n",
    "    print(item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on Doc object:\n",
      "\n",
      "class Doc(builtins.object)\n",
      " |  A sequence of Token objects. Access sentences and named entities, export\n",
      " |  annotations to numpy arrays, losslessly serialize to compressed binary\n",
      " |  strings. The `Doc` object holds an array of `TokenC` structs. The\n",
      " |  Python-level `Token` and `Span` objects are views of this array, i.e.\n",
      " |  they don't own the data themselves.\n",
      " |  \n",
      " |  EXAMPLE: Construction 1\n",
      " |      >>> doc = nlp(u'Some text')\n",
      " |  \n",
      " |      Construction 2\n",
      " |      >>> from spacy.tokens import Doc\n",
      " |      >>> doc = Doc(nlp.vocab, words=[u'hello', u'world', u'!'],\n",
      " |                    spaces=[True, False, False])\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __bytes__(...)\n",
      " |  \n",
      " |  __getitem__(...)\n",
      " |      Get a `Token` or `Span` object.\n",
      " |      \n",
      " |      i (int or tuple) The index of the token, or the slice of the document\n",
      " |          to get.\n",
      " |      RETURNS (Token or Span): The token at `doc[i]]`, or the span at\n",
      " |          `doc[start : end]`.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> doc[i]\n",
      " |          Get the `Token` object at position `i`, where `i` is an integer.\n",
      " |          Negative indexing is supported, and follows the usual Python\n",
      " |          semantics, i.e. `doc[-2]` is `doc[len(doc) - 2]`.\n",
      " |      \n",
      " |          >>> doc[start : end]]\n",
      " |          Get a `Span` object, starting at position `start` and ending at\n",
      " |          position `end`, where `start` and `end` are token indices. For\n",
      " |          instance, `doc[2:5]` produces a span consisting of tokens 2, 3 and\n",
      " |          4. Stepped slices (e.g. `doc[start : end : step]`) are not\n",
      " |          supported, as `Span` objects must be contiguous (cannot have gaps).\n",
      " |          You can use negative indices and open-ended ranges, which have\n",
      " |          their normal Python semantics.\n",
      " |  \n",
      " |  __init__(...)\n",
      " |      Create a Doc object.\n",
      " |      \n",
      " |      vocab (Vocab): A vocabulary object, which must match any models you\n",
      " |          want to use (e.g. tokenizer, parser, entity recognizer).\n",
      " |      words (list or None): A list of unicode strings to add to the document\n",
      " |          as words. If `None`, defaults to empty list.\n",
      " |      spaces (list or None): A list of boolean values, of the same length as\n",
      " |          words. True means that the word is followed by a space, False means\n",
      " |          it is not. If `None`, defaults to `[True]*len(words)`\n",
      " |      user_data (dict or None): Optional extra data to attach to the Doc.\n",
      " |      RETURNS (Doc): The newly constructed object.\n",
      " |  \n",
      " |  __iter__(...)\n",
      " |      Iterate over `Token`  objects, from which the annotations can be\n",
      " |      easily accessed. This is the main way of accessing `Token` objects,\n",
      " |      which are the main way annotations are accessed from Python. If faster-\n",
      " |      than-Python speeds are required, you can instead access the annotations\n",
      " |      as a numpy array, or access the underlying C data directly from Cython.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> for token in doc\n",
      " |  \n",
      " |  __len__(...)\n",
      " |      The number of tokens in the document.\n",
      " |      \n",
      " |      RETURNS (int): The number of tokens in the document.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> len(doc)\n",
      " |  \n",
      " |  __new__(*args, **kwargs) from builtins.type\n",
      " |      Create and return a new object.  See help(type) for accurate signature.\n",
      " |  \n",
      " |  __reduce__ = __reduce_cython__(...)\n",
      " |  \n",
      " |  __repr__(self, /)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__ = __setstate_cython__(...)\n",
      " |  \n",
      " |  __str__(self, /)\n",
      " |      Return str(self).\n",
      " |  \n",
      " |  __unicode__(...)\n",
      " |  \n",
      " |  char_span(...)\n",
      " |      Create a `Span` object from the slice `doc.text[start : end]`.\n",
      " |      \n",
      " |      doc (Doc): The parent document.\n",
      " |      start (int): The index of the first character of the span.\n",
      " |      end (int): The index of the first character after the span.\n",
      " |      label (uint64 or string): A label to attach to the Span, e.g. for\n",
      " |          named entities.\n",
      " |      vector (ndarray[ndim=1, dtype='float32']): A meaning representation of\n",
      " |          the span.\n",
      " |      RETURNS (Span): The newly constructed object.\n",
      " |  \n",
      " |  count_by(...)\n",
      " |      Count the frequencies of a given attribute. Produces a dict of\n",
      " |      `{attribute (int): count (ints)}` frequencies, keyed by the values of\n",
      " |      the given attribute ID.\n",
      " |      \n",
      " |      attr_id (int): The attribute ID to key the counts.\n",
      " |      RETURNS (dict): A dictionary mapping attributes to integer counts.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> from spacy import attrs\n",
      " |          >>> doc = nlp(u'apple apple orange banana')\n",
      " |          >>> tokens.count_by(attrs.ORTH)\n",
      " |          {12800L: 1, 11880L: 2, 7561L: 1}\n",
      " |          >>> tokens.to_array([attrs.ORTH])\n",
      " |          array([[11880], [11880], [7561], [12800]])\n",
      " |  \n",
      " |  extend_tensor(...)\n",
      " |      Concatenate a new tensor onto the doc.tensor object.\n",
      " |      \n",
      " |      The doc.tensor attribute holds dense feature vectors\n",
      " |      computed by the models in the pipeline. Let's say a\n",
      " |      document with 30 words has a tensor with 128 dimensions\n",
      " |      per word. doc.tensor.shape will be (30, 128). After\n",
      " |      calling doc.extend_tensor with an array of hape (30, 64),\n",
      " |      doc.tensor == (30, 192).\n",
      " |  \n",
      " |  from_array(...)\n",
      " |  \n",
      " |  from_bytes(...)\n",
      " |      Deserialize, i.e. import the document contents from a binary string.\n",
      " |      \n",
      " |      data (bytes): The string to load from.\n",
      " |      RETURNS (Doc): Itself.\n",
      " |  \n",
      " |  from_disk(...)\n",
      " |      Loads state from a directory. Modifies the object in place and\n",
      " |      returns it.\n",
      " |      \n",
      " |      path (unicode or Path): A path to a directory. Paths may be either\n",
      " |          strings or `Path`-like objects.\n",
      " |      RETURNS (Doc): The modified `Doc` object.\n",
      " |  \n",
      " |  get_extension(...) from builtins.type\n",
      " |  \n",
      " |  get_lca_matrix(...)\n",
      " |      Calculates the lowest common ancestor matrix for a given `Doc`.\n",
      " |      Returns LCA matrix containing the integer index of the ancestor, or -1\n",
      " |      if no common ancestor is found (ex if span excludes a necessary\n",
      " |      ancestor). Apologies about the recursion, but the impact on\n",
      " |      performance is negligible given the natural limitations on the depth\n",
      " |      of a typical human sentence.\n",
      " |  \n",
      " |  has_extension(...) from builtins.type\n",
      " |  \n",
      " |  merge(...)\n",
      " |      Retokenize the document, such that the span at\n",
      " |      `doc.text[start_idx : end_idx]` is merged into a single token. If\n",
      " |      `start_idx` and `end_idx `do not mark start and end token boundaries,\n",
      " |      the document remains unchanged.\n",
      " |      \n",
      " |      start_idx (int): Character index of the start of the slice to merge.\n",
      " |      end_idx (int): Character index after the end of the slice to merge.\n",
      " |      **attributes: Attributes to assign to the merged token. By default,\n",
      " |          attributes are inherited from the syntactic root of the span.\n",
      " |      RETURNS (Token): The newly merged token, or `None` if the start and end\n",
      " |          indices did not fall at token boundaries.\n",
      " |  \n",
      " |  print_tree(...)\n",
      " |      Returns the parse trees in JSON (dict) format.\n",
      " |      \n",
      " |      light (bool): Don't include lemmas or entities.\n",
      " |      flat (bool): Don't include arcs or modifiers.\n",
      " |      RETURNS (dict): Parse tree as dict.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> doc = nlp('Bob brought Alice the pizza. Alice ate the pizza.')\n",
      " |          >>> trees = doc.print_tree()\n",
      " |          >>> trees[1]\n",
      " |          {'modifiers': [\n",
      " |              {'modifiers': [], 'NE': 'PERSON', 'word': 'Alice',\n",
      " |              'arc': 'nsubj', 'POS_coarse': 'PROPN', 'POS_fine': 'NNP',\n",
      " |              'lemma': 'Alice'},\n",
      " |              {'modifiers': [\n",
      " |                  {'modifiers': [], 'NE': '', 'word': 'the', 'arc': 'det',\n",
      " |                  'POS_coarse': 'DET', 'POS_fine': 'DT', 'lemma': 'the'}],\n",
      " |              'NE': '', 'word': 'pizza', 'arc': 'dobj', 'POS_coarse': 'NOUN',\n",
      " |              'POS_fine': 'NN', 'lemma': 'pizza'},\n",
      " |              {'modifiers': [], 'NE': '', 'word': '.', 'arc': 'punct',\n",
      " |              'POS_coarse': 'PUNCT', 'POS_fine': '.', 'lemma': '.'}],\n",
      " |              'NE': '', 'word': 'ate', 'arc': 'ROOT', 'POS_coarse': 'VERB',\n",
      " |              'POS_fine': 'VBD', 'lemma': 'eat'}\n",
      " |  \n",
      " |  retokenize(...)\n",
      " |      Context manager to handle retokenization of the Doc.\n",
      " |      Modifications to the Doc's tokenization are stored, and then\n",
      " |      made all at once when the context manager exits. This is\n",
      " |      much more efficient, and less error-prone.\n",
      " |      \n",
      " |      All views of the Doc (Span and Token) created before the\n",
      " |      retokenization are invalidated, although they may accidentally\n",
      " |      continue to work.\n",
      " |  \n",
      " |  set_extension(...) from builtins.type\n",
      " |  \n",
      " |  similarity(...)\n",
      " |      Make a semantic similarity estimate. The default estimate is cosine\n",
      " |      similarity using an average of word vectors.\n",
      " |      \n",
      " |      other (object): The object to compare with. By default, accepts `Doc`,\n",
      " |          `Span`, `Token` and `Lexeme` objects.\n",
      " |      RETURNS (float): A scalar similarity score. Higher is more similar.\n",
      " |  \n",
      " |  to_array(...)\n",
      " |      Export given token attributes to a numpy `ndarray`.\n",
      " |      If `attr_ids` is a sequence of M attributes, the output array will be\n",
      " |      of shape `(N, M)`, where N is the length of the `Doc` (in tokens). If\n",
      " |      `attr_ids` is a single attribute, the output shape will be (N,). You\n",
      " |      can specify attributes by integer ID (e.g. spacy.attrs.LEMMA) or\n",
      " |      string name (e.g. 'LEMMA' or 'lemma').\n",
      " |      \n",
      " |      attr_ids (list[]): A list of attributes (int IDs or string names).\n",
      " |      RETURNS (numpy.ndarray[long, ndim=2]): A feature matrix, with one row\n",
      " |          per word, and one column per attribute indicated in the input\n",
      " |          `attr_ids`.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> from spacy.attrs import LOWER, POS, ENT_TYPE, IS_ALPHA\n",
      " |          >>> doc = nlp(text)\n",
      " |          >>> # All strings mapped to integers, for easy export to numpy\n",
      " |          >>> np_array = doc.to_array([LOWER, POS, ENT_TYPE, IS_ALPHA])\n",
      " |  \n",
      " |  to_bytes(...)\n",
      " |      Serialize, i.e. export the document contents to a binary string.\n",
      " |      \n",
      " |      RETURNS (bytes): A losslessly serialized copy of the `Doc`, including\n",
      " |          all annotations.\n",
      " |  \n",
      " |  to_disk(...)\n",
      " |      Save the current state to a directory.\n",
      " |      \n",
      " |      path (unicode or Path): A path to a directory, which will be created if\n",
      " |          it doesn't exist. Paths may be either strings or Path-like objects.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  cats\n",
      " |  \n",
      " |  doc\n",
      " |  \n",
      " |  ents\n",
      " |      Iterate over the entities in the document. Yields named-entity\n",
      " |      `Span` objects, if the entity recognizer has been applied to the\n",
      " |      document.\n",
      " |      \n",
      " |      YIELDS (Span): Entities in the document.\n",
      " |      \n",
      " |      EXAMPLE: Iterate over the span to get individual Token objects,\n",
      " |          or access the label:\n",
      " |      \n",
      " |          >>> tokens = nlp(u'Mr. Best flew to New York on Saturday morning.')\n",
      " |          >>> ents = list(tokens.ents)\n",
      " |          >>> assert ents[0].label == 346\n",
      " |          >>> assert ents[0].label_ == 'PERSON'\n",
      " |          >>> assert ents[0].orth_ == 'Best'\n",
      " |          >>> assert ents[0].text == 'Mr. Best'\n",
      " |  \n",
      " |  has_vector\n",
      " |      A boolean value indicating whether a word vector is associated with\n",
      " |      the object.\n",
      " |      \n",
      " |      RETURNS (bool): Whether a word vector is associated with the object.\n",
      " |  \n",
      " |  is_parsed\n",
      " |  \n",
      " |  is_sentenced\n",
      " |  \n",
      " |  is_tagged\n",
      " |  \n",
      " |  mem\n",
      " |  \n",
      " |  noun_chunks\n",
      " |      Iterate over the base noun phrases in the document. Yields base\n",
      " |      noun-phrase #[code Span] objects, if the document has been\n",
      " |      syntactically parsed. A base noun phrase, or \"NP chunk\", is a noun\n",
      " |      phrase that does not permit other NPs to be nested within it â€“ so no\n",
      " |      NP-level coordination, no prepositional phrases, and no relative\n",
      " |      clauses.\n",
      " |      \n",
      " |      YIELDS (Span): Noun chunks in the document.\n",
      " |  \n",
      " |  noun_chunks_iterator\n",
      " |  \n",
      " |  sentiment\n",
      " |  \n",
      " |  sents\n",
      " |      Iterate over the sentences in the document. Yields sentence `Span`\n",
      " |      objects. Sentence spans have no label. To improve accuracy on informal\n",
      " |      texts, spaCy calculates sentence boundaries from the syntactic\n",
      " |      dependency parse. If the parser is disabled, the `sents` iterator will\n",
      " |      be unavailable.\n",
      " |      \n",
      " |      EXAMPLE:\n",
      " |          >>> doc = nlp(\"This is a sentence. Here's another...\")\n",
      " |          >>> assert [s.root.text for s in doc.sents] == [\"is\", \"'s\"]\n",
      " |  \n",
      " |  tensor\n",
      " |  \n",
      " |  text\n",
      " |      A unicode representation of the document text.\n",
      " |      \n",
      " |      RETURNS (unicode): The original verbatim text of the document.\n",
      " |  \n",
      " |  text_with_ws\n",
      " |      An alias of `Doc.text`, provided for duck-type compatibility with\n",
      " |      `Span` and `Token`.\n",
      " |      \n",
      " |      RETURNS (unicode): The original verbatim text of the document.\n",
      " |  \n",
      " |  user_data\n",
      " |  \n",
      " |  user_hooks\n",
      " |  \n",
      " |  user_span_hooks\n",
      " |  \n",
      " |  user_token_hooks\n",
      " |  \n",
      " |  vector\n",
      " |      A real-valued meaning representation. Defaults to an average of the\n",
      " |      token vectors.\n",
      " |      \n",
      " |      RETURNS (numpy.ndarray[ndim=1, dtype='float32']): A 1D numpy array\n",
      " |          representing the document's semantics.\n",
      " |  \n",
      " |  vector_norm\n",
      " |      The L2 norm of the document's vector representation.\n",
      " |      \n",
      " |      RETURNS (float): The L2 norm of the vector representation.\n",
      " |  \n",
      " |  vocab\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __pyx_vtable__ = <capsule object NULL>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "UnicodeDecodeError",
     "evalue": "'ascii' codec can't decode byte 0x82 in position 6: ordinal not in range(128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-98-35684d01c21d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data/bailin_data/pre-trained-glove.pkl'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m             \u001b[0mvectors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mUnicodeDecodeError\u001b[0m: 'ascii' codec can't decode byte 0x82 in position 6: ordinal not in range(128)"
     ]
    }
   ],
   "source": [
    "with open('data/bailin_data/pre-trained-glove.pkl', 'rb') as f:\n",
    "            vectors = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from model_glove import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from data/2014/pre-trained-glove.pkl with shape (5135, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardsun/anaconda3/envs/allennlp/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "model = attTSA(config)\n",
    "parameters = filter(lambda p: p.requires_grad, model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_data(data_path, if_utf=False):\n",
    "    f = open(data_path, 'rb')\n",
    "    obj = pickle.load(f)\n",
    "    f.close()\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_batch, test_batch = load_data('data/bailin_data/data.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7678571428571429"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "860/len(test_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for triple_list in test_batch:\n",
    "        model.zero_grad() \n",
    "        if len(triple_list) == 0: \n",
    "            continue\n",
    "        ##Modified by Richard Sun\n",
    "        sent, mask, label = triple_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 10, 64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "b = tf.Variable(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
