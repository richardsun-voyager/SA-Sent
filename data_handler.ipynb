{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple, defaultdict\n",
    "import codecs\n",
    "from config import config\n",
    "from bs4 import BeautifulSoup\n",
    "import pdb\n",
    "import torch\n",
    "from nltk.tokenize import word_tokenize\n",
    "#import tokenizer\n",
    "import numpy as np\n",
    "import re\n",
    "import pickle\n",
    "import random\n",
    "##Added by Richard Sun\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "from data_reader import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DATA_PATH = \"data/2014/Restaurants_Train_v2.xml\"\n",
    "TEST_DATA_PATH = \"data/2014/Restaurants_Test_Gold.xml\"\n",
    "TRAIN_DATA_PATH = \"data/Indonesian/indo_tweets.csv\"\n",
    "\n",
    "# TRAIN_DATA_PATH = \"./data/2014/Laptop_Train_v2.xml\"\n",
    "# TEST_DATA_PATH = \"./data/2014/Laptops_Test_Gold.xml\"\n",
    "\n",
    "GLOVE_FILE = \"../data/word_embeddings/indo_vectors.txt\"\n",
    "OUT_FILE = config.embed_path\n",
    "DATA_FILE = config.data_path\n",
    "DIC_FILE = config.dic_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install xlrd\n",
    "import pandas as pd\n",
    "#help(pd.read_excel)\n",
    "data = pd.read_csv(TRAIN_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count        7132\n",
       "unique          3\n",
       "top       Neutral\n",
       "freq         3091\n",
       "Name: Sentiment, dtype: object"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Sentiment.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Username</th>\n",
       "      <th>Keyword</th>\n",
       "      <th>Tweet</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Overal Sentiment</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6130</th>\n",
       "      <td>6130</td>\n",
       "      <td>ZUL_Hasan</td>\n",
       "      <td>ISIS</td>\n",
       "      <td>TIPS jadi saksi nikah : Jangan baper .. Sesama...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>remarks: the ISIS in this context does not ref...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6534</th>\n",
       "      <td>6534</td>\n",
       "      <td>tifsembiring</td>\n",
       "      <td>ISIS</td>\n",
       "      <td>ISIS ...? ada yg bilang \"Istri Sholihah Idaman...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>remarks: ISIS here does not refer to the Islam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6538</th>\n",
       "      <td>6538</td>\n",
       "      <td>tifsembiring</td>\n",
       "      <td>HTI</td>\n",
       "      <td>@rayestu kmbalikan pd hti nurani mas, kita kom...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>remarks: hti nurani = hati nurani, does not re...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0      Username Keyword  \\\n",
       "6130        6130     ZUL_Hasan    ISIS   \n",
       "6534        6534  tifsembiring    ISIS   \n",
       "6538        6538  tifsembiring     HTI   \n",
       "\n",
       "                                                  Tweet Sentiment  \\\n",
       "6130  TIPS jadi saksi nikah : Jangan baper .. Sesama...       NaN   \n",
       "6534  ISIS ...? ada yg bilang \"Istri Sholihah Idaman...       NaN   \n",
       "6538  @rayestu kmbalikan pd hti nurani mas, kita kom...       NaN   \n",
       "\n",
       "     Overal Sentiment                                         Unnamed: 5  \n",
       "6130              NaN  remarks: the ISIS in this context does not ref...  \n",
       "6534              NaN  remarks: ISIS here does not refer to the Islam...  \n",
       "6538              NaN  remarks: hti nurani = hati nurani, does not re...  "
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[pd.isnull(data[['Sentiment']]).any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.to_csv('data/Indonesian/indo_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dr = data_reader(config)\n",
    "dr.load_data(config.train_path)\n",
    "dr_valid = data_reader(config, False)\n",
    "dr_valid.load_data(config.valid_path)\n",
    "dr_test = data_reader(config, False)\n",
    "dr_test.load_data(config.test_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = dr.generate_sample(dr.data_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['ss', 'ff','df','gf']\n",
    "word_freq_pair = Counter(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {2: 805, 0: 2164, 1: 633})\n"
     ]
    }
   ],
   "source": [
    "data_batch = dh.to_batches(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset number: 606\n",
      "Target error meal\n",
      "Local Embeddings Saved!\n"
     ]
    }
   ],
   "source": [
    "data2 = dh.read(TEST_DATA_PATH, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = reader.generate_sample(train_batch)\n",
    "sent_vecs, mask_vecs, label_list, sent_lens = elmo_transform(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_vecs, mask_vecs, label_list, sent_lens = elmo_transform([test_batch[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write to pkl\n",
    "with open(DATA_FILE, \"wb\") as f:\n",
    "    pickle.dump([train_batch, test_batch],f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "with codecs.open(DIC_FILE, 'wb') as f:\n",
    "    pickle.dump(reader.id2word, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1411 unk out of 5138 vocab\n"
     ]
    }
   ],
   "source": [
    "reader.gen_vectors_glove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/richardsun/anaconda3/envs/allennlp/lib/python3.6/site-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  \"num_layers={}\".format(dropout, num_layers))\n"
     ]
    }
   ],
   "source": [
    "from model_att import *\n",
    "model = attTSA(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:To Head:be Children: []\n",
      "Text:be Head:was Children: [To, fair]\n",
      "Text:completely Head:fair Children: []\n",
      "Text:fair Head:be Children: [completely]\n",
      "Text:, Head:was Children: []\n",
      "Text:the Head:factor Children: []\n",
      "Text:only Head:factor Children: []\n",
      "Text:redeeming Head:factor Children: []\n",
      "Text:factor Head:was Children: [the, only, redeeming]\n",
      "Text:was Head:was Children: [be, ,, factor, food, ,, but, make, .]\n",
      "Text:the Head:food Children: []\n",
      "Text:food Head:was Children: [the, ,, was]\n",
      "Text:, Head:food Children: []\n",
      "Text:which Head:was Children: []\n",
      "Text:was Head:food Children: [which, above]\n",
      "Text:above Head:was Children: [average]\n",
      "Text:average Head:above Children: []\n",
      "Text:, Head:was Children: []\n",
      "Text:but Head:was Children: []\n",
      "Text:could Head:make Children: []\n",
      "Text:n't Head:make Children: []\n",
      "Text:make Head:was Children: [could, n't, up, for]\n",
      "Text:up Head:make Children: []\n",
      "Text:for Head:make Children: [deficiencies]\n",
      "Text:all Head:deficiencies Children: []\n",
      "Text:the Head:deficiencies Children: []\n",
      "Text:other Head:deficiencies Children: []\n",
      "Text:deficiencies Head:for Children: [all, the, other, of]\n",
      "Text:of Head:deficiencies Children: [Teodora]\n",
      "Text:Teodora Head:of Children: []\n",
      "Text:. Head:was Children: []\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u\"To be completely fair, the only redeeming factor was the food, which was above average, but couldn't make up for all the other deficiencies of Teodora.\")\n",
    "target = 'food'\n",
    "for token in doc:\n",
    "#     if 'food' in token.head.text:\n",
    "#         print(token)\n",
    "#     children = [t.text for t in token.children]    \n",
    "#     if 'food' in children:\n",
    "#         print(token)\n",
    "    print('Text:'+ token.text, 'Head:'+token.head.text, 'Children:',\n",
    "           [child for child in token.children])\n",
    "#     if token.text == 'food':\n",
    "#         a = token\n",
    "#     if token.text == 'deficiencies':\n",
    "#         b= token\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be completely fair, the only redeeming factor was the food, which was above average, but couldn't make up for all the other deficiencies of Teodora.\n",
      "*******\n"
     ]
    }
   ],
   "source": [
    "for item in doc.sents:\n",
    "    print(item)\n",
    "    print('*******')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text:the Head:screen Children: []\n",
      "Text:screen Head:is Children: [the]\n",
      "Text:is Head:is Children: [screen, good, but, sucks]\n",
      "Text:good Head:is Children: []\n",
      "Text:but Head:is Children: []\n",
      "Text:the Head:battery Children: []\n",
      "Text:battery Head:sucks Children: [the]\n",
      "Text:sucks Head:is Children: [battery]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(u'the screen is good but the battery sucks')\n",
    "for token in doc:\n",
    "#     if 'food' in token.head.text:\n",
    "#         print(token)\n",
    "#     children = [t.text for t in token.children]    \n",
    "#     if 'food' in children:\n",
    "#         print(token)\n",
    "    print('Text:'+ token.text, 'Head:'+token.head.text, 'Children:',\n",
    "           [child for child in token.children])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" id=\"0\" class=\"displacy\" width=\"1450\" height=\"399.5\" style=\"max-width: none; height: 399.5px; color: #000000; background: #ffffff; font-family: Arial\">\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"225\">screen</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"225\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"400\">is</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"400\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"575\">good</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"575\">ADJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"750\">but</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"750\">CCONJ</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"925\">the</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"925\">DET</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1100\">battery</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1100\">NOUN</tspan>\n",
       "</text>\n",
       "\n",
       "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"309.5\">\n",
       "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1275\">sucks</tspan>\n",
       "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1275\">VERB</tspan>\n",
       "</text>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-0\" stroke-width=\"2px\" d=\"M70,264.5 C70,177.0 215.0,177.0 215.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-0\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M70,266.5 L62,254.5 78,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-1\" stroke-width=\"2px\" d=\"M245,264.5 C245,177.0 390.0,177.0 390.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-1\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M245,266.5 L237,254.5 253,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-2\" stroke-width=\"2px\" d=\"M420,264.5 C420,177.0 565.0,177.0 565.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-2\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">acomp</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M565.0,266.5 L573.0,254.5 557.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-3\" stroke-width=\"2px\" d=\"M420,264.5 C420,89.5 745.0,89.5 745.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-3\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M745.0,266.5 L753.0,254.5 737.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-4\" stroke-width=\"2px\" d=\"M945,264.5 C945,177.0 1090.0,177.0 1090.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-4\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M945,266.5 L937,254.5 953,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-5\" stroke-width=\"2px\" d=\"M1120,264.5 C1120,177.0 1265.0,177.0 1265.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-5\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1120,266.5 L1112,254.5 1128,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "\n",
       "<g class=\"displacy-arrow\">\n",
       "    <path class=\"displacy-arc\" id=\"arrow-0-6\" stroke-width=\"2px\" d=\"M420,264.5 C420,2.0 1275.0,2.0 1275.0,264.5\" fill=\"none\" stroke=\"currentColor\"/>\n",
       "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
       "        <textPath xlink:href=\"#arrow-0-6\" class=\"displacy-label\" startOffset=\"50%\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
       "    </text>\n",
       "    <path class=\"displacy-arrowhead\" d=\"M1275.0,266.5 L1283.0,254.5 1267.0,254.5\" fill=\"currentColor\"/>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style='dep', jupyter=True)\n",
    "# trees = doc.print_tree()\n",
    "# trees[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Layer import SimpleCat\n",
    "from data_reader_general import dataHelper, data_generator\n",
    "#from model_glove import *\n",
    "from model_crf import *\n",
    "from config import config\n",
    "cat_layer = SimpleCat(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cat_layer.load_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent = 'To heal the hearts of believers and to enrage kuffar, \\\n",
    "munafikeen and murtadeen, we present the last video memoir of Shaheed Eisa Fazili. May Allah accept him into highest Jannah'\n",
    "targets = ['Eisa Fazili', 'kuffar', 'munafikeen', 'murtadeen', 'Allah']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sent = \"But the staff was so good to us\"\n",
    "targets = ['staff']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_list = []\n",
    "opinion_list = []\n",
    "SentInst = namedtuple(\"SentenceInstance\", \"id text text_ids text_inds opinions\")\n",
    "OpinionInst = namedtuple(\"OpinionInstance\", \"target_text polarity class_ind target_mask target_ids\")\n",
    "for target in targets:\n",
    "    opinion_inst = OpinionInst(target, 'negative', None, None, None)\n",
    "    opinion_list.append(opinion_inst)\n",
    "sent_Inst = SentInst(1, sent, None, None, opinion_list)\n",
    "sentence_list.append(sent_Inst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "dh = dataHelper(config)\n",
    "data, words = dh.process_raw_data(sentence_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'int'>, {2: 5})\n"
     ]
    }
   ],
   "source": [
    "data_batch = dh.to_batches(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dg = data_generator(config, data_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_vecs, mask_vecs, label_list, sent_lens = dg.elmo_transform(data_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = AspectSent(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label, best_seq = model.predict(sent_vecs, mask_vecs, sent_lens) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0,  0,  0,  0])"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores, s_probs, best_seqs = model.compute_predict_scores(sent_vecs, mask_vecs, sent_lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.6032, -4.7932,  0.9438],\n",
       "        [ 2.6839, -5.2023,  1.0347],\n",
       "        [ 2.7342, -5.2895,  1.0273],\n",
       "        [ 2.7248, -5.2792,  1.0532],\n",
       "        [ 2.7875, -5.3021,  1.0744]])"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8397,  0.0005,  0.1598],\n",
       "        [ 0.8385,  0.0003,  0.1612],\n",
       "        [ 0.8462,  0.0003,  0.1535],\n",
       "        [ 0.8415,  0.0003,  0.1582],\n",
       "        [ 0.8470,  0.0003,  0.1527]])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "F.softmax(scores, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_reader import data_reader\n",
    "dr = data_reader(config)\n",
    "dr.load_data(config.train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3001"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dr.data_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_list, mask_list, label_list, _ = zip(*dr.data_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(label_list, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1816,  531,  654])"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2])"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [[0,1,1,0],[0,1,0], [0,0,0,1,1]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "b = torch.zeros(3, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    temp = a[i]\n",
    "    b[i, :len(temp)] = torch.FloatTensor(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a=torch.LongTensor([2,5,7,3,4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_lens, perm_idx = a.sort(0, descending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(7)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[perm_idx[0].item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 7,  5,  4,  3,  2])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_lens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from model_att_glove import *\n",
    "from data_reader_general import *\n",
    "from config import config\n",
    "import pickle\n",
    "from Layer import GloveMaskCat\n",
    "import numpy as np\n",
    "import codecs\n",
    "import copy\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving successfully!\n"
     ]
    }
   ],
   "source": [
    "save_path = 'data/22222222222222222222222.txt'\n",
    "data = np.random.randn(10,20)\n",
    "with open(save_path, \"wb\") as f:\n",
    "    pickle.dump(data,f)\n",
    "print('Saving successfully!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dr = data_reader(config)\n",
    "train_data = dr.load_data(config.data_path+'Restaurants_Train_v2.xml.pkl')\n",
    "test_data = dr.load_data(config.data_path+'Restaurants_Test_Gold.xml.pkl')\n",
    "valid_data = dr.load_data(config.data_path+'valid_restaurant.pkl')\n",
    "dg_train = data_generator(config, train_data)\n",
    "dg_valid = data_generator(config, valid_data, False)\n",
    "dg_test =data_generator(config, test_data, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded from data/2014/vocab/local_emb.pkl with shape (4334, 300)\n"
     ]
    }
   ],
   "source": [
    "cat_layer = GloveMaskCat(config)\n",
    "cat_layer.load_vector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_vecs, mask_vecs, label_list, sent_lens, tokens = next(dg_valid.get_ids_samples())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
       "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_vecs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   52,  2175,    11,  2176,   220,   102,    10,     3,   134,\n",
       "          983,  2177,     3,   451,  2178,     1,  2179,    11,   152,\n",
       "          834,   233,   218,    44,   260,    32,  1482,    37,     4,\n",
       "          511,    32,  2180,  2181,    38,     6,   116,  1184,    13,\n",
       "            1,   141,   107,     4,   984,     2])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_vecs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_attention(dr_test, model):\n",
    "    print(\"Evaluting\")\n",
    "    dr_test.reset_samples()\n",
    "    model.eval()\n",
    "    all_counter = 0\n",
    "    correct_count = 0\n",
    "    while dr_test.index < dr_test.data_len:\n",
    "        sent, mask, label, sent_len, tokens = next(dr_test.get_ids_samples())\n",
    "        sent, target = cat_layer(sent, mask)\n",
    "        if config.if_gpu: \n",
    "            sent, target = sent.cuda(), target.cuda()\n",
    "            label, sent_len = label.cuda(), sent_len.cuda()\n",
    "        pred_label, attentions  = model.predict(sent, target, sent_len) \n",
    "        print(tokens[:3])\n",
    "        print(mask[:3])\n",
    "        print(attentions[:3])\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluting\n",
      "[['an', 'oasis', 'of', 'refinement', ':', ' ', 'food', ',', 'though', 'somewhat', 'uneven', ',', 'often', 'reaches', 'the', 'pinnacles', 'of', 'new', 'american', 'fine', 'cuisine', '  ', 'chef', \"'s\", 'passion', '(', 'and', 'kitchen', \"'s\", 'precise', 'execution', ')', 'is', 'most', 'evident', 'in', 'the', 'fish', 'dishes', 'and', 'soups', '.'], ['the', 'food', 'can', 'get', 'pricey', 'but', 'the', 'prixe', 'fixe', 'tasting', 'menu', 'is', 'the', 'greatest', 'food', 'for', 'a', 'good', 'price', 'and', 'they', 'cater', 'the', 'food', 'to', 'any', 'food', 'allergies', 'or', 'food', 'you', 'do', \"n't\", 'like', '.'], ['the', 'red', 'curry', 'is', 'weak', 'and', 'tasteless', ',', 'the', 'pad', 'thai', 'is', 'stuck', 'together', 'and', 'lumpy', ',', 'the', 'rice', 'is', 'often', 'overcooked', ',', 'and', 'the', 'seafood', 'is', 'pretty', 'sketchy', '.']]\n",
      "tensor([[ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0],\n",
      "        [ 0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  1,  0,  0,\n",
      "          0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]])\n",
      "tensor(1.00000e-02 *\n",
      "       [[[ 0.4153,  0.4311,  0.4129,  0.4147,  0.4132,  0.6607,  3.0489,\n",
      "           3.0490,  3.0490,  3.0490,  3.0490,  3.0490,  3.0490,  3.0490,\n",
      "           3.0490,  3.0490,  3.0489,  2.6214,  0.6024,  0.4271,  1.8440,\n",
      "           0.5916,  2.9807,  2.0470,  2.5822,  1.2471,  3.0465,  3.0490,\n",
      "           3.0490,  3.0490,  3.0490,  3.0490,  3.0490,  3.0490,  3.0490,\n",
      "           3.0490,  3.0490,  3.0490,  3.0490,  3.0490,  3.0490,  3.0367]],\n",
      "\n",
      "        [[ 0.5859,  0.6752,  0.5979,  1.3908,  4.3227,  4.3293,  4.3293,\n",
      "           4.3293,  4.3294,  4.3294,  4.3294,  4.3294,  4.3292,  4.3294,\n",
      "           4.3294,  4.3294,  4.3294,  4.3294,  4.3294,  4.3294,  4.3292,\n",
      "           4.2334,  0.5992,  0.7310,  0.5859,  0.5859,  0.5860,  0.5859,\n",
      "           0.5859,  0.5859,  0.5859,  0.5859,  0.5859,  0.5859,  0.5859,\n",
      "           1.5927,  1.5927,  1.5927,  1.5927,  1.5927,  1.5927,  1.5927]],\n",
      "\n",
      "        [[ 0.8920,  2.9054,  4.1142,  4.1341,  4.1525,  4.1484,  4.1332,\n",
      "           2.4000,  0.5727,  0.5698,  0.6120,  0.5734,  0.6592,  1.5546,\n",
      "           1.6337,  1.3939,  1.6087,  0.6152,  2.1349,  2.2245,  3.6625,\n",
      "           4.1788,  4.1770,  4.1785,  3.6205,  4.1797,  4.0064,  4.1742,\n",
      "           4.1774,  4.1543,  1.5382,  1.5382,  1.5382,  1.5382,  1.5382,\n",
      "           1.5382,  1.5382,  1.5382,  1.5382,  1.5382,  1.5382,  1.5382]]])\n"
     ]
    }
   ],
   "source": [
    "model_file = config.model_path+'model.pt'\n",
    "if os.path.exists(model_file):\n",
    "    model = torch.load(model_file)\n",
    "    visualize_attention(dg_valid, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "emb_dim= 300\n",
    "def load_pretrained_word_emb(file_path):\n",
    "    '''\n",
    "    Load a specified vocabulary\n",
    "    '''\n",
    "    word_emb = {}\n",
    "    vocab_words = set()\n",
    "    with open(file_path) as fi:\n",
    "        for line in fi:\n",
    "            items = line.split()\n",
    "            word = ' '.join(items[:-1*emb_dim])\n",
    "            vec = items[-1*emb_dim:]\n",
    "            word_emb[word] = np.array(vec, dtype=np.float32)\n",
    "            vocab_words.add(word)\n",
    "    return word_emb, vocab_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-c7389998d5a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'../data/glove.840B.300d.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_pretrained_word_emb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-37-ec37d2ba14c9>\u001b[0m in \u001b[0;36mload_pretrained_word_emb\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m     14\u001b[0m             \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0memb_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m             \u001b[0mvec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mitems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0memb_dim\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m             \u001b[0mword_emb\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m             \u001b[0mvocab_words\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "file = '../data/glove.840B.300d.txt'\n",
    "a,b = load_pretrained_word_emb(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "a ='hello -0.1573 -0.29517 0.30453 -0.54773 0.098293 -0.1776 0.21662 0.19261 -0.21101 0.53788 -0.047755 0.40675 0.023592 -0.32814 0.046858 0.19367 0.25565 -0.021019 -0.15957 -0.1023 0.20303 -0.043333 0.11618 -0.18486 0.0011948 -0.052301 0.34587 0.052335 0.16774 -0.21384 0.055947 0.24934 -0.12179 0.16749 0.28922 -0.033739 0.3015 -0.13241 0.092635 0.37155 -0.2884 -0.0052731 -0.001005 -0.51153 -0.28476 -0.20139 0.11837 -0.0055891 0.43604 0.16796 -0.2701 0.063957 -0.093253 -0.22079 0.36501 0.06545 0.23941 -0.19292 0.098293 0.12172 -0.1168 -0.027436 0.20507 -0.39139 -0.23111 0.46239 0.22888 -0.028415 -0.1798 0.23817 0.28093 -0.47935 0.23177 -0.35587 0.14246 0.11861 0.011018 0.091986 0.0054809 -0.39955 -0.40183 -0.10629 -0.30851 0.12383 -0.16737 -0.43569 0.4211 -0.57416 -0.19964 0.51312 0.090747 -0.21657 0.043519 0.24288 0.081134 0.49104 -0.33342 -0.31056 -0.3136 0.26931 -0.14402 0.33185 -0.21662 -0.072985 0.080603 -0.7266 -0.098385 -0.36233 -0.25346 0.1154 0.25738 0.15802 -0.15633 -0.024581 0.35673 0.31153 0.33475 -0.081155 -0.3061 0.019077 -0.049047 -0.11232 -0.07417 0.35596 -0.2642 0.012781 -0.20715 0.020223 0.054534 -0.28803 0.42863 -0.10312 0.24771 0.013196 0.19768 -0.013528 -0.15134 0.20307 -0.028973 -0.022706 -0.29199 -0.082062 0.19048 0.0053574 0.14067 -0.28675 0.21343 0.42428 -0.28186 -0.11801 -0.45227 -0.0067998 0.044784 -0.0062886 0.25087 0.34481 -0.64459 -0.20467 0.35007 0.1468 -0.14007 -0.0050219 -0.24053 0.41426 -0.40902 0.21141 0.25726 -0.4883 0.027066 0.56367 -0.39594 -0.035206 0.63079 0.14343 0.038315 0.32527 -0.080335 -0.20065 -0.30848 -0.0031591 0.15296 -0.21014 0.42143 -0.20944 -0.069285 0.13555 -0.020401 -0.22555 0.33491 0.16035 0.17739 -0.023627 0.097575 -0.19395 -0.018754 -0.119 -0.0067027 -0.4178 0.29027 0.13034 -0.30212 0.61173 -0.39918 -0.020191 -0.34531 -0.092082 0.46818 0.36671 0.21021 -0.053162 -0.37872 -0.14271 -0.13604 0.31715 -0.17227 -0.091266 0.16417 0.15069 0.53556 -0.29678 0.13965 -0.29788 0.1282 0.1971 -0.045515 -0.41355 -0.050333 -0.39015 -0.29579 -0.096145 -0.03151 0.053714 -0.37309 -0.36523 -0.17235 0.39251 -0.065909 -0.25267 -0.34448 -0.11503 0.43665 0.18832 0.20631 0.27801 -0.046077 0.13397 -0.091953 -0.098542 0.15811 0.2752 0.081383 0.32077 -0.10028 0.1088 -0.24836 0.10477 0.15243 -0.071302 0.12861 0.23061 0.0074864 0.090918 -0.12269 -0.14831 0.010586 0.35745 -0.23412 -0.23746 -0.22646 -0.27641 -0.1634 0.071909 -0.093884 0.21331 -0.20627 0.44406 0.34691 0.019064 0.034657 0.36789 0.32276 -0.31099 -0.023443 -0.77048 -0.26001 0.033961 -0.13874 0.051973 -0.0090509 0.27427 0.046548 -0.48214 -0.1437 -0.1975 -0.038126 -0.16555 0.071697 0.049449 0.15386 -0.81663'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'allennlp'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-26cacadecef4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmodel_parse_glove\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_reader_general\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mparse_path\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdependency_path\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mconfigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_parse\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/SA-Sent/data_reader_general.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;31m##Added by Richard Sun\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mallennlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melmo\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mElmo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_to_ids\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mnlp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0men_core_web_sm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'allennlp'"
     ]
    }
   ],
   "source": [
    "from model_parse_glove import *\n",
    "from data_reader_general import *\n",
    "from parse_path import dependency_path\n",
    "from configs.config_parse import config\n",
    "import pickle\n",
    "from Layer import GloveMaskCat\n",
    "import numpy as np\n",
    "import codecs\n",
    "import copy\n",
    "import os, sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parse_path import dependency_path\n",
    "dp = dependency_path()\n",
    "import torch\n",
    "def get_dependency_weight(tokens, targets):\n",
    "    max_len = max(map(len, tokens))\n",
    "    weights = np.zeros([len(tokens), max_len])\n",
    "    for i, token in enumerate(tokens):\n",
    "        #print('Original word num')\n",
    "        #print(len(token))\n",
    "        print(i)\n",
    "        text = ' '.join(token)#Connect them into a string\n",
    "\n",
    "        graph = dp.build_graph(text)\n",
    "        mat = dp.compute_node_distance(graph)\n",
    "        #print(len(new_tokens))\n",
    "        if len(token) != len(mat):\n",
    "            print('Word number conflicts!')\n",
    "\n",
    "        try:\n",
    "            max_w, _, _ = dp.compute_soft_targets_weights(mat, targets[i])\n",
    "            weights[i, :len(max_w)] = max_w\n",
    "        except:\n",
    "            print('text process error')\n",
    "            print(text, targets[i])\n",
    "            break\n",
    "    return torch.FloatTensor(weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [['i', 'understand', 'the', 'area', 'and', 'folks', 'you', 'need', 'not', 'come', 'here', 'for', 'the', 'romantic', ',', 'alluring', 'ambiance', 'or', 'the', 'five', 'star', 'service', 'featuring', 'a', 'sommlier', 'and', 'a', 'complicated', 'maze', 'of', 'captain', 'and', 'back', 'waiters', 'you', 'come', 'for', 'the', 'authentic', 'foods', ',', 'the', 'tastes', ',', 'the', 'experiance', '.'], ['the', 'dishes', 'offered', 'were', 'unique', ',', 'very', 'tasty', 'and', 'fresh', 'from', 'the', 'lamb', 'sausages', ',', 'sardines', 'with', 'biscuits', ',', 'large', 'whole', 'shrimp', 'to', 'the', 'amazing', 'pistachio', 'ice', 'cream', '(', 'the', 'best', 'and', 'freshest', 'i', \"'ve\", 'ever', 'had', ')', '.'], ['i', 'am', 'a', '100', 'lb', 'girl', ',', 'had', 'a', 'glass', 'of', 'wine', 'and', 'a', 'glass', 'of', 'beer', 'prior', 'to', 'the', 'dinner', ',', 'and', 'i', 'was', 'still', 'hungry', 'after', 'my', 'visit', 'to', 'this', 'place'], ['the', 'people', 'with', 'carts', 'of', 'food', 'do', \"n't\", 'understand', 'you', 'because', 'they', 'do', \"n't\", 'speak', 'english', ',', 'their', 'job', 'is', 'to', 'give', 'you', 'the', 'delicious', 'food', 'you', 'point', 'at', '.'], ['i', 'ordered', 'tamarind', 'duck','and', 'my', 'wife', 'ordered', 'noodles', 'with', 'ground', 'beef', ',', 'and', 'we', 'were', 'both', 'delighted', 'by', 'the', 'way', 'the', 'dishes', 'evoked', 'thai', 'flavors', 'in', 'unexpected', 'ways', '.'], ['if', 'you', \"'re\", 'looking', 'for', 'perfect', 'traditional', 'sushi', ',', 'go', 'here', 'if', 'you', \"'re\", 'looking', 'for', 'interesting', 'combinations', ',', 'try', 'sushi', 'of', 'gari', \"'s\", '(', 'east', 'side', ')', '.'], ['i', 'have', 'lived', 'in', 'japan', 'for', '7', 'years', 'and', 'the', 'taste', 'of', 'the', 'food', 'and', 'the', 'feel', 'of', 'the', 'restaurant', 'is', 'like', 'being', 'back', 'in', 'japan', '.'], ['yes', ',', 'they', 'use', 'fancy', 'ingredients', ',', 'but', 'even', 'fancy', 'ingredients', 'do', \"n't\", 'make', 'for', 'good', 'pizza', 'unless', 'someone', 'knows', 'how', 'to', 'get', 'the', 'crust', 'right', '.'], ['we', 'recently', 'decided', 'to', 'try', 'this', 'location', ',', 'and', 'to', 'our', 'delight', ',', 'they', 'have', 'outdoor', 'seating', ',', 'perfect', 'since', 'i', 'had', 'my', 'yorkie', 'with', 'me', '.'], ['succulent', 'steaks', 'cooked', 'precisely', 'to', 'your', 'desired', \"'\", 'doneness', \"'\", 'accompanied', 'by', 'salads', 'and', 'sides', 'that', 'do', \"n't\", 'look', 'like', 'leafy', 'road', 'kill', '.'], ['i', 'usually', 'go', 'there', 'later', 'at', 'night', 'when', 'i', 'get', 'off', 'work', 'so', 'i', 'do', \"n't\", 'have', 'to', 'deal', 'with', 'crowds', 'or', 'lines', '.'], ['the', 'space', 'is', 'a', 'bit', 'too', 'small', 'for', 'live', 'music', ',', 'so', 'on', 'jazz', 'nights', ',', 'it', 'can', 'be', 'loud', 'and', 'cramped', '.'], ['the', 'real', 'kicker', 'of', 'the', 'menu', ',', 'however', ',', 'is', 'the', 'beef', 'cubes', 'or', 'the', 'chicken', 'with', 'chili', 'and', 'lemon', 'grass', '.'], ['waiters', 'tend', 'to', 'forget', 'drinks', 'completely', ',', 'food', 'portions', 'are', 'so', 'tiny', ',', 'two', 'people', 'have', 'trouble', 'sharing', 'one', 'entree', '.'], ['any', 'if', 'you', 'have', 'a', 'reservation', 'you', \"'ll\", 'wait', 'for', 'max', '5', 'minutes', 'so', 'have', 'a', 'drink', 'at', 'the', 'bar', '.'], ['a', 'restaurant', 'that', 'does', \"n't\", 'try', 'to', 'do', 'anything', 'except', 'serve', 'great', 'food', 'with', 'great', 'service', 'in', 'a', 'pleasant', 'atmosphere', '.'], ['the', 'in', 'house', 'lady', 'dj', 'on', 'saturday', 'nights', 'has', 'outrageously', 'good', 'taste', 'in', 'music', ',', 'and', 'moreover', ',', 'takes', 'requests', '.'], ['the', 'wine', 'is', 'always', 'good', ',', 'the', 'tapas', 'are', 'always', 'yummy', ',', 'especially', 'with', 'the', 'warm', 'pita', 'bread', '.'], ['only', 'drawback', 'they', 'wo', \"n't\", 'toast', 'your', 'bagel', ',', 'and', 'they', 'do', \"n't\", 'make', 'eggs', 'for', 'the', 'bagel', '.'], ['one', 'of', 'my', 'favorites', 'though', 'was', 'the', 'angry', 'lobster', ',', 'a', 'cold', 'lobster', 'salad', 'that', 'was', 'magnificent', '.'], ['the', 'food', 'is', 'wonderful', ',', 'tasty', 'and', 'filling', ',', 'and', 'the', 'service', 'is', 'professional', 'and', 'friendly', '.'], ['overall', 'the', 'restaurant', 'is', 'more', 'expensive', 'than', 'our', 'other', 'sushi', 'favorites', ',', 'but', 'everything', 'was', 'delicious', '.'], ['try', 'the', 'pad', 'se', 'ew', 'or', 'chicken', 'with', 'cashew', 'nuts', 'for', 'a', 'memorable', 'and', 'repeatable', 'experience', '.'], ['food', 'is', 'average', ',', 'and', 'i', 'would', 'say', 'even', 'the', 'chain', 'restaurant', 'baluchi', \"'s\", 'tastes', 'better', '.'], ['after', 'my', '3rd', 'time', 'the', 'manager', 'remembered', 'me', 'and', 'treated', 'me', 'like', 'an', 'usual', 'customer', '.'], ['they', 'would', 'nt', 'even', 'let', 'me', 'finish', 'my', 'glass', 'of', 'wine', 'before', 'offering', 'another', '.'], ['they', 'even', 'have', 'a', 'section', 'in', 'the', 'menu', 'called', 'american', 'chinese', 'food'], ['the', 'wait', 'staff', 'is', 'very', 'friendly', ',', 'if', 'not', 'overly', 'efficient', '.'], ['i', 'could', \"n't\", 'reccommend', 'their', 'godmother', 'pizza', 'any', 'higher', '.'], ['yeah', ',', 'sometimes', 'the', 'service', 'can', 'be', 'slow', '.'], ['tasty', 'steak', ',', 'pork', 'loin', ',', 'the', 'works', '.'], ['service', 'friendly', 'and', 'attentive', '.']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = [[1,2]]*20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 'Great food, great prices, great service.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "spanlp = spacy.load('en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_mask_index(masks):\n",
    "    '''\n",
    "    Find the indice of none zeros values in masks, namely the target indice\n",
    "    '''\n",
    "    target_indice = []\n",
    "    for mask in masks:\n",
    "        indice = torch.nonzero(mask == 1).squeeze(1).numpy()\n",
    "        target_indice.append(indice)\n",
    "    return target_indice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NodeView(('owner-1', 'same-0', 'as-2', ',-8', 'is-10', '.-21', 'guy-4', 'the-3', 'owns-6', 'who-5', 'typhoon-7', 'which-9', 'down-12', 'just-11', 'street-14', 'the-13', 'on-15', 'marks-17', 'st.-16', 'and-18', 'ave-20', '1st-19'))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import networkx as nx\n",
    "text = \"same owner as the guy who owns typhoon , which is just down the street on St. marks and 1st ave .\"\n",
    "document = spanlp(text.replace(\" '\", \"'\"))\n",
    "edges = []\n",
    "for token in document:\n",
    "    # FYI https://spacy.io/docs/api/token\n",
    "    for child in token.children:\n",
    "        edges.append(('{0}-{1}'.format(token.lower_,token.i),\n",
    "                    '{0}-{1}'.format(child.lower_,child.i)))\n",
    "\n",
    "graph = nx.Graph(edges)\n",
    "graph.nodes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NN] ||| transplant ||| transplantation ||| PPDB2.0Score=5.24981 PPDB1.0Score=3.295900 -logp(LHS|e1)=0.18597 -logp(LHS|e2)=0.14031 -logp(e1|LHS)=11.83583 -logp(e1|e2)=1.80507 -logp(e1|e2,LHS)=1.46728 -logp(e2|LHS)=11.47593 -logp(e2|e1)=1.49083 -logp(e2|e1,LHS)=1.10738 AGigaSim=0.63439 Abstract=0 Adjacent=0 CharCountDiff=5 CharLogCR=0.40547 ContainsX=0 Equivalence=0.371472 Exclusion=0.000344 GlueRule=0 GoogleNgramSim=0.03067 Identity=0 Independent=0.078161 Lex(e1|e2)=9.64663 Lex(e2|e1)=59.48919 Lexical=1 LogCount=4.67283 MVLSASim=NA Monotonic=1 OtherRelated=0.372735 PhrasePenalty=1 RarityPenalty=0 ForwardEntailment=0.177287 SourceTerminalsButNoTarget=0 SourceWords=1 TargetComplexity=0.98821 TargetFormality=0.98464 TargetTerminalsButNoSource=0 TargetWords=1 UnalignedSource=0 UnalignedTarget=0 WordCountDiff=0 WordLenDiff=5.00000 WordLogCR=0 ||| 0-0 ||| OtherRelated\n",
      "\n",
      "[JJ] ||| <www.un.org/depts/dgacm/docs/crp/aconf212crp1/russian.pdf> ||| <www.un.org/depts/dgacm/docs/crp/aconf212crp1/arabic.pdf> ||| PPDB2.0Score=5.22098 PPDB1.0Score=9.463800 -logp(LHS|e1)=0 -logp(LHS|e2)=0 -logp(e1|LHS)=15.67499 -logp(e1|e2)=4.73190 -logp(e1|e2,LHS)=2.18813 -logp(e2|LHS)=15.67499 -logp(e2|e1)=4.73190 -logp(e2|e1,LHS)=2.18813 AGigaSim=0 Abstract=0 Adjacent=0 CharCountDiff=-1 CharLogCR=-0.01739 ContainsX=0 Equivalence=0.630569 Exclusion=0.000045 GlueRule=0 GoogleNgramSim=0 Identity=0 Independent=0.257275 Lex(e1|e2)=61.80282 Lex(e2|e1)=61.80282 Lexical=1 LogCount=1.79176 MVLSASim=NA Monotonic=1 OtherRelated=0.086572 PhrasePenalty=1 RarityPenalty=0 ForwardEntailment=0.025539 SourceTerminalsButNoTarget=0 SourceWords=1 TargetTerminalsButNoSource=0 TargetWords=1 UnalignedSource=0 UnalignedTarget=0 WordCountDiff=0 WordLenDiff=-1.00000 WordLogCR=0 ||| 0-0 ||| Equivalence\n",
      "\n",
      "[JJ] ||| <www.un.org/depts/dgacm/docs/crp/aconf212crp1/russian.pdf> ||| <www.un.org/depts/dgacm/docs/crp/aconf212crp1/french.pdf> ||| PPDB2.0Score=5.21828 PPDB1.0Score=9.463820 -logp(LHS|e1)=0 -logp(LHS|e2)=0 -logp(e1|LHS)=15.67499 -logp(e1|e2)=4.73191 -logp(e1|e2,LHS)=2.18812 -logp(e2|LHS)=15.67499 -logp(e2|e1)=4.73191 -logp(e2|e1,LHS)=2.18812 AGigaSim=0 Abstract=0 Adjacent=0 CharCountDiff=-1 CharLogCR=-0.01739 ContainsX=0 Equivalence=0.628765 Exclusion=0.000039 GlueRule=0 GoogleNgramSim=0 Identity=0 Independent=0.258367 Lex(e1|e2)=61.80282 Lex(e2|e1)=61.80282 Lexical=1 LogCount=1.79176 MVLSASim=NA Monotonic=1 OtherRelated=0.087278 PhrasePenalty=1 RarityPenalty=0 ForwardEntailment=0.025551 SourceTerminalsButNoTarget=0 SourceWords=1 TargetTerminalsButNoSource=0 TargetWords=1 UnalignedSource=0 UnalignedTarget=0 WordCountDiff=0 WordLenDiff=-1.00000 WordLogCR=0 ||| 0-0 ||| Equivalence\n",
      "\n",
      "[JJ] ||| <www.un.org/depts/dgacm/docs/crp/aconf212crp1/french.pdf> ||| <www.un.org/depts/dgacm/docs/crp/aconf212crp1/russian.pdf> ||| PPDB2.0Score=5.21462 PPDB1.0Score=9.463780 -logp(LHS|e1)=0 -logp(LHS|e2)=0 -logp(e1|LHS)=15.67499 -logp(e1|e2)=4.73189 -logp(e1|e2,LHS)=2.18812 -logp(e2|LHS)=15.67499 -logp(e2|e1)=4.73189 -logp(e2|e1,LHS)=2.18812 AGigaSim=0 Abstract=0 Adjacent=0 CharCountDiff=1 CharLogCR=0.01739 ContainsX=0 Equivalence=0.628765 Exclusion=0.000039 GlueRule=0 GoogleNgramSim=0 Identity=0 Independent=0.258367 Lex(e1|e2)=61.80282 Lex(e2|e1)=61.80282 Lexical=1 LogCount=1.79176 MVLSASim=NA Monotonic=1 OtherRelated=0.087278 PhrasePenalty=1 RarityPenalty=0 ReverseEntailment=0.025551 SourceTerminalsButNoTarget=0 SourceWords=1 TargetTerminalsButNoSource=0 TargetWords=1 UnalignedSource=0 UnalignedTarget=0 WordCountDiff=0 WordLenDiff=1.00000 WordLogCR=0 ||| 0-0 ||| Equivalence\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('data/ppdb-2.0-s-all') as f:\n",
    "    i =0 \n",
    "    for line in f:\n",
    "        i += 1\n",
    "        print(line)\n",
    "        if i>3:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['[NN] ',\n",
       " ' transplant ',\n",
       " ' transplantation ',\n",
       " ' PPDB2.0Score=5.24981 PPDB1.0Score=3.295900 -logp(LHS|e1)=0.18597 -logp(LHS|e2)=0.14031 -logp(e1|LHS)=11.83583 -logp(e1|e2)=1.80507 -logp(e1|e2,LHS)=1.46728 -logp(e2|LHS)=11.47593 -logp(e2|e1)=1.49083 -logp(e2|e1,LHS)=1.10738 AGigaSim=0.63439 Abstract=0 Adjacent=0 CharCountDiff=5 CharLogCR=0.40547 ContainsX=0 Equivalence=0.371472 Exclusion=0.000344 GlueRule=0 GoogleNgramSim=0.03067 Identity=0 Independent=0.078161 Lex(e1|e2)=9.64663 Lex(e2|e1)=59.48919 Lexical=1 LogCount=4.67283 MVLSASim=NA Monotonic=1 OtherRelated=0.372735 PhrasePenalty=1 RarityPenalty=0 ForwardEntailment=0.177287 SourceTerminalsButNoTarget=0 SourceWords=1 TargetComplexity=0.98821 TargetFormality=0.98464 TargetTerminalsButNoSource=0 TargetWords=1 UnalignedSource=0 UnalignedTarget=0 WordCountDiff=0 WordLenDiff=5.00000 WordLogCR=0 ',\n",
       " ' 0-0 ',\n",
       " ' OtherRelated\\n']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "line.split('|||')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
